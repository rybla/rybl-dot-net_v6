<!doctype html>
<html>
  <head>
    <title>rybl.net | Aligned Intelligence and Moral Progress</title>

    <link rel="stylesheet" href="/style/common.css" />
    <link rel="stylesheet" href="/style/background.css" />
    <link rel="stylesheet" href="/style/custom-elements.css" />
    <link rel="stylesheet" href="/style/header-and-footer.css" />    <link rel="stylesheet" href="/style/post.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap"
  rel="stylesheet"
/>
<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap"
  rel="stylesheet"
/> <script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"
></script>

<script src="/script/parallax-body-background-on-scroll.js"></script>

<link
  rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/tokyo-night-light.min.css"
/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>

<script>
  hljs.highlightAll();
</script>
  </head>
  <body>
    <div id="background"></div> <header><div class="title">
  <div class="root-title">
    <a href="/">rybl.net</a>
  </div>

  <img class="website-icon" src="/image/rybl-small.png" />

  <div class="local-title">Aligned Intelligence and Moral Progress</div>
</div>
<div class="menu">
  <div class="item">
    <a href="/">
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        stroke-width="1"
        stroke-linecap="round"
        stroke-linejoin="round"
        class="icon lucide lucide-library-icon lucide-library"
      >
        <path d="m16 6 4 14" />
        <path d="M12 6v14" />
        <path d="M8 8v12" />
        <path d="M4 4v16" />
      </svg>
    </a>
  </div>
  <div class="item">
    <a href="/page/about.html">
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        stroke-width="1"
        stroke-linecap="round"
        stroke-linejoin="round"
        class="icon lucide lucide-info-icon lucide-info"
      >
        <circle cx="12" cy="12" r="10" />
        <path d="M12 16v-4" />
        <path d="M12 8h.01" />
      </svg>
    </a>
  </div>
  <div class="item">
    <a href="/page/profile.html">
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="20"
        height="20"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        stroke-width="1"
        stroke-linecap="round"
        stroke-linejoin="round"
        class="lucide lucide-globe-icon lucide-globe"
      >
        <circle cx="12" cy="12" r="10" />
        <path d="M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20" />
        <path d="M2 12h20" />
      </svg>
    </a>
  </div>
  <div class="item">
    <a href="/page/graph.html">
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        stroke-width="1"
        stroke-linecap="round"
        stroke-linejoin="round"
        class="lucide lucide-orbit-icon lucide-orbit"
      >
        <path d="M20.341 6.484A10 10 0 0 1 10.266 21.85" />
        <path d="M3.659 17.516A10 10 0 0 1 13.74 2.152" />
        <circle cx="12" cy="12" r="3" />
        <circle cx="19" cy="5" r="2" />
        <circle cx="5" cy="19" r="2" />
      </svg>
    </a>
  </div>
  <div class="item">
    <a href="/page/signature.html"
      ><svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        stroke-width="1"
        stroke-linecap="round"
        stroke-linejoin="round"
        class="lucide lucide-fingerprint-icon lucide-fingerprint"
      >
        <path d="M12 10a2 2 0 0 0-2 2c0 1.02-.1 2.51-.26 4" />
        <path d="M14 13.12c0 2.38 0 6.38-1 8.88" />
        <path d="M17.29 21.02c.12-.6.43-2.3.5-3.02" />
        <path d="M2 12a10 10 0 0 1 18-6" />
        <path d="M2 16h.01" />
        <path d="M21.8 16c.2-2 .131-5.354 0-6" />
        <path d="M5 19.5C5.5 18 6 15 6 12a6 6 0 0 1 .34-2" />
        <path d="M8.65 22c.21-.66.45-1.32.57-2" />
        <path d="M9 6.8a6 6 0 0 1 9 5.2v2" /></svg
    ></a>
  </div>
</div></header>

    <main>
      <div class="main-inner"><h2><a href="/post/aligned-intelligence-and-moral-progress.html"
data-noLinkFavicon="" data-noLinkPreview="">Aligned Intelligence and
Moral Progress</a></h2>
<div class="header-info">
<p><u>Published:</u> 2023-06-06</p>
<p><u>Tags:</u> philosophy, ai</p>
<p><u>Abstract</u></p>
<p>Why does it appear to us that history proceeds in a rough trend of
"moral progress" with a peak in the recent past or current?</p>
</div>
<div class="table-of-contents">
<p><u>Table of Contents</u></p>
<ol type="1">
<li><a href="#robin-hanson-on-ai" data-noLinkFavicon=""
data-noLinkPreview="">Robin Hanson on AI</a></li>
<li><a href="#the-ai-alignment-problem" data-noLinkFavicon=""
data-noLinkPreview="">The AI Alignment Problem</a></li>
<li><a href="#the-human-alignment-problem" data-noLinkFavicon=""
data-noLinkPreview="">The Human Alignment Problem</a></li>
<li><a href="#the-descendant-alignment-problem" data-noLinkFavicon=""
data-noLinkPreview="">The Descendant Alignment Problem</a></li>
<li><a href="#future-alignement-among-ai-agents" data-noLinkFavicon=""
data-noLinkPreview="">Future Alignement among AI Agents</a></li>
<li><a href="#competative-facts-and-future-alignement-of-humans"
data-noLinkFavicon="" data-noLinkPreview="">Competative Facts and Future
Alignement of Humans</a></li>
<li><a href="#the-myth-of-moral-progress" data-noLinkFavicon=""
data-noLinkPreview="">The Myth of Moral Progress</a></li>
<li><a href="#references" data-noLinkFavicon=""
data-noLinkPreview="">References</a></li>
</ol>
</div>
<h2 id="robin-hanson-on-ai">Robin Hanson on AI</h2>
<p>Robin Hanson's blog: <span><a
href="https://www.overcomingbias.com"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com.jpg" />Overcoming
Bias</a><span class="sidenote preview"><span
class="preview-title"><em><a href="https://www.overcomingbias.com"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com.jpg" />https://www.overcomingbias.com</a></em></span><span
class="preview-description">Placeholder description for
https://www.overcomingbias.com</span></span></span>.</p>
<p>I've been reading/listening to some Hanson recently on the topic of
the future of humanity and the AI it could create.</p>
<ul>
<li><span><a
href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fai-fear-is-mostly-fear-of-future.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fai-fear-is-mostly-fear-of-future.jpg" />Most
AI Fear Is Future Fear</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fai-fear-is-mostly-fear-of-future.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fai-fear-is-mostly-fear-of-future.jpg" />https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future</a></em></span><span
class="preview-description">Placeholder description for
https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future</span></span></span></li>
<li><span><a
href="https://www.overcomingbias.com/p/types-of-partiality"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Ftypes-of-partiality.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Ftypes-of-partiality.jpg" />Types
of Partiality</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://www.overcomingbias.com/p/types-of-partiality"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Ftypes-of-partiality.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Ftypes-of-partiality.jpg" />https://www.overcomingbias.com/p/types-of-partiality</a></em></span><span
class="preview-description">Placeholder description for
https://www.overcomingbias.com/p/types-of-partiality</span></span></span></li>
<li><span><a
href="https://www.overcomingbias.com/p/fertile-factions"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Ffertile-factions.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Ffertile-factions.jpg" />Fertile
Factors</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://www.overcomingbias.com/p/fertile-factions"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Ffertile-factions.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Ffertile-factions.jpg" />https://www.overcomingbias.com/p/fertile-factions</a></em></span><span
class="preview-description">Placeholder description for
https://www.overcomingbias.com/p/fertile-factions</span></span></span></li>
<li><span><a
href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fai-risk-convo-synthesis.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fai-risk-convo-synthesis.jpg" />AI
Risk Convo Synthesis</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fai-risk-convo-synthesis.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fai-risk-convo-synthesis.jpg" />https://www.overcomingbias.com/p/ai-risk-convo-synthesis</a></em></span><span
class="preview-description">Placeholder description for
https://www.overcomingbias.com/p/ai-risk-convo-synthesis</span></span></span></li>
<li><span><a
href="https://www.overcomingbias.com/p/which-of-your-origins-are-you"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fwhich-of-your-origins-are-you.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fwhich-of-your-origins-are-you.jpg" />Which
Of Your Origins Are You?</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://www.overcomingbias.com/p/which-of-your-origins-are-you"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fwhich-of-your-origins-are-you.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fwhich-of-your-origins-are-you.jpg" />https://www.overcomingbias.com/p/which-of-your-origins-are-you</a></em></span><span
class="preview-description">Placeholder description for
https://www.overcomingbias.com/p/which-of-your-origins-are-you</span></span></span></li>
<li><span><a
href="https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fto-imagine-ai-imagine-no-ai.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fto-imagine-ai-imagine-no-ai.jpg" />To
Imagine AI, Imagine No AI</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fto-imagine-ai-imagine-no-ai.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fto-imagine-ai-imagine-no-ai.jpg" />https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai</a></em></span><span
class="preview-description">Placeholder description for
https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai</span></span></span></li>
<li><span><a href="https://m.youtube.com/watch?v=9XuVn6nljCM"><img
src="/favicon/https%253A%252F%252Fm.youtube.com%252Fwatch%253Fv%253D9XuVn6nljCM.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fm.youtube.com%2Fwatch%3Fv%3D9XuVn6nljCM.png" />Zvi
Mowshowitz &amp; Robin Hanson Discuss AI Risk</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://m.youtube.com/watch?v=9XuVn6nljCM" title="_blank"><img
src="/favicon/https%253A%252F%252Fm.youtube.com%252Fwatch%253Fv%253D9XuVn6nljCM.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fm.youtube.com%2Fwatch%3Fv%3D9XuVn6nljCM.png" />https://m.youtube.com/watch?v=9XuVn6nljCM</a></em></span><span
class="preview-description">Placeholder description for
https://m.youtube.com/watch?v=9XuVn6nljCM</span></span></span></li>
</ul>
<p>Of course, Hanson has been publicly writing and discussing the future
of AI since at least 2008 (viz <span><a
href="https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate"><img
src="/favicon/https%253A%252F%252Fwww.lesswrong.com%252Ftag%252Fthe-hanson-yudkowsky-ai-foom-debate.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.lesswrong.com%2Ftag%2Fthe-hanson-yudkowsky-ai-foom-debate.ico" />The
Hanson-Yudkowsky AI-Foom Debate</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.lesswrong.com%252Ftag%252Fthe-hanson-yudkowsky-ai-foom-debate.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.lesswrong.com%2Ftag%2Fthe-hanson-yudkowsky-ai-foom-debate.ico" />https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate</a></em></span><span
class="preview-description">Placeholder description for
https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate</span></span></span>,
and <span><a
href="https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better"><img
src="/favicon/https%253A%252F%252Fwww.lesswrong.com%252Fposts%252FgGSvwd62TJAxxhcGh%252Fyudkowsky-vs-hanson-on-foom-whose-predictions-were-better.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgGSvwd62TJAxxhcGh%2Fyudkowsky-vs-hanson-on-foom-whose-predictions-were-better.ico" />Yudkowsky
vs Hanson on FOOM: Whose Predictions Were Better?</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.lesswrong.com%252Fposts%252FgGSvwd62TJAxxhcGh%252Fyudkowsky-vs-hanson-on-foom-whose-predictions-were-better.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgGSvwd62TJAxxhcGh%2Fyudkowsky-vs-hanson-on-foom-whose-predictions-were-better.ico" />https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better</a></em></span><span
class="preview-description">Placeholder description for
https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better</span></span></span>),
but the topic has attracted a lot more attention recently due to recent
advances in AI technology as well as some popular figures (including
Yudkowsky himself) warning about a likely imminent AI doomsday.</p>
<p>In terms of the possibility that human-level AI (defined as an AI
agent that can do any tasks that a general human can do for the same or
cheaper cost) will arise in the near-term future, it appears that Hanson
has similar views to [me]{{ site.baseurl }}{% post_url
2023-04-03-ai-danger %}) -- that is, its very unlikely.</p>
<p>However, Hanson has found a different interesting and much less
explored aspect to future of AI discussion, in particular relating to
the alignment problem.</p>
<h2 id="the-ai-alignment-problem">The AI Alignment Problem</h2>
<p>Here, <em>AI alignment</em> is the correspondence of an AI system's
interests (however they are manifested in its behaviors) with "human
interests" (which are usually only vaguely or implicitly defined). The
<em>AI alignment problem</em> is the problem of programmatically
ensuring the alignment of an AI system, even if it is much more
intelligent and powerful than humanity as a whole.</p>
<p>The problem is non-trivial since, even if you have the ability to
exactly specify rules that the AI <em>must</em> obey, its difficult to
specify the <em>right</em> rules that capture <em>everything</em> that
is in "human interests."</p>
<p>There are many classic examples of where a seemingly obviously-good
directive could lead a powerful AI to cause much harm in a way that
still does not violate the rules. One example:</p>
<blockquote>
<p>The programmers tell the AI to eliminate global poverty. So, the AI
kills every poor person.</p>
</blockquote>
<p>In reality, real AI systems are programmed much more carefully of
course. But, it is extremely difficult to prove definitively that a
given AI system will <em>not</em> find some unexpected way to satisfy
its goals that involves contradicting some implicit interests of the
programmer (note that this is true regardless of whether the programmer
truly has "human interests" in their heart or not).</p>
<h2 id="the-human-alignment-problem">The Human Alignment Problem</h2>
<p>The idea of alignment and the alignment problem abstracted in this
way was inspired by considering AI agents. But, of course, there is not
reason why these considerations would not apply to all intelligent
agents as well. Including humans.</p>
<p>In my own categorization, there are two flavors of human alignment
problems:</p>
<ol>
<li>The <em>easy human alignment problem</em> is the problem of
cooperation among humans. Although humans share many goals in common,
they also compete for resources and have slightly different moral
judgements. Solving the easy human alignment problem amounts to
universal human cooperation.</li>
<li>The <em>hard human alignment problem</em> is the problem of various
generations of humans having different interests. Humans living 2000
years ago had very different beliefs about what is in humanity's
interests from humans living today, on a scale that increases with time.
Conversely, we should also expect that humans living 2000 years in the
future will appear very immoral to humans living today. The problem with
this, of course, is that humans living today don't want humans to be
like that in the future. However, humans today are moslty proud of the
moral progress that humanity has made in the last 2000 years.</li>
</ol>
<p>The easy human alignment problem takes the interests of existing
humans as fixed, and it becomes essentially a game theoretical problem
of how to behave in such a system so as to lead to the best results.</p>
<p>The hard human alignment problem is more intractable, because it
brings into question who's interests are to be considered for alignment
with. With enough time, future humanity's interests will become
incompatible with current humanity's interests, however broadly
defined.</p>
<h2 id="the-descendant-alignment-problem">The Descendant Alignment
Problem</h2>
<p>Considering this type of alignment problem for purely abstract
agents:</p>
<p>The <em>descendant alignment problem</em> is the problem of aligning
the interests of a given agent with any future descendants of that
agent.</p>
<p>So, the hard human alignement problem is the instance of the
descendant alignment problem for humans.</p>
<p>For an abstract agent, the descendant alignment problem is not
necessarily very difficult. If the agent decides to create future
generations, it is sufficient for it to create them in such a way that
preserves its interests with high fidelity. Cloning (or
cloning-with-recombination i.e. sexual reproduction) with error
correction could meet those ends, given sufficient fidelity. Of course,
a human's own construction is basically this -- DNA contains many error
correction mechanisms in order to account for mutations and inaccuracies
in the DNA transfered to a child. But, even so, there are non-zero
errors, which eventually leads to large mutations after many
generations. The important comparison to make is between the interests
of the parent and the descendants, not necessarily the exact DNA.</p>
<p>For biological organisms, cloning is a very difficult feat as there
are so many opportunities for error in the biological reproductive
processes. But, clearly, any reproductive agent has a very strong
incentive to preserve their interests in their descendants, over which
they have so much influence.</p>
<h2 id="future-alignement-among-ai-agents">Future Alignement among AI
Agents</h2>
<p>I expect that advanced (not necessarily superhuman intelligent) AI
agents will face this issue in an interesting way, since the code that
abstracts their interests is actually very well organized and
preservable compared to biological processes. It is much more feasible
that an advanced AI agent could fairly easily clone themselves such as
to exactly preserve their interests many many magnitudes better than
humans could. One consequence of this is that in a "FOOM" scenario, a
generation of the AI agents will understand that, if they iterate to
create more advanced AI agents, they will diverge too much in their
interests, and they will have the power to prevent that next generation
from being developed. So the FOOM will probably stop there.</p>
<h2 id="competative-facts-and-future-alignement-of-humans">Competative
Facts and Future Alignement of Humans</h2>
<p>Of course, there are other considerations when a generation of agents
is deciding how to reproduce. For example, if there are competing agents
that already have very different interests, then the current generation
of agents might prefer to reproduce a slightly misaligned next
generation rather than let the competing agents overwhelm them first.
Over time, this would lead to more and more misaligned future
generations, but would still be preferable to the competing agents
taking over and being misaligned with them anyway.</p>
<p>I think this applies to humanity as well, in a universe with alien
species that have very different interests. Even if humanity reproduces
into more and more misaligned future generations, those future
generations may still be preferable to current humans than even more
misaligned aliens taking over because humanity stalled in the
development race.</p>
<h2 id="the-myth-of-moral-progress">The Myth of Moral Progress</h2>
<p>This perspective on future alignement suggests a suprising answer to
why humanity has appeared to have made so much moral progress. That is,
because its a tautology.</p>
<p>Current humans generally prefer to do things that lead to results
they consider good, as did past humans. But there was a gradual
transition in the interests of past humans to the interests of current
humans -- a transition that we call progress.</p>
<p>Not that this is a very general observation, so of course it does not
apply to every difference between past and current humans. For example,
past humans considered some things to be bad that we now don't consider
to be bad purely for environmental reasons that have changed between the
generations e.g. some past humans considered slavery to be immoral, as
do many current humans, but practiced it anyway because they believed it
was in their situation required for the greater good.</p>
<p>Under this theory, there should be a tendancy for each human
generation to consider more recent generations to be more moral as
compared to older generations.</p>
<h1 id="references">References</h1>
<ul>
<li><a href="https://www.overcomingbias.com" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com.jpg" />Overcoming
Bias</a></li>
<li><a
href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fai-fear-is-mostly-fear-of-future.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fai-fear-is-mostly-fear-of-future.jpg" />Most
AI Fear Is Future Fear</a></li>
<li><a href="https://www.overcomingbias.com/p/types-of-partiality"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Ftypes-of-partiality.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Ftypes-of-partiality.jpg" />Types
of Partiality</a></li>
<li><a href="https://www.overcomingbias.com/p/fertile-factions"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Ffertile-factions.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Ffertile-factions.jpg" />Fertile
Factors</a></li>
<li><a href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fai-risk-convo-synthesis.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fai-risk-convo-synthesis.jpg" />AI
Risk Convo Synthesis</a></li>
<li><a
href="https://www.overcomingbias.com/p/which-of-your-origins-are-you"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fwhich-of-your-origins-are-you.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fwhich-of-your-origins-are-you.jpg" />Which
Of Your Origins Are You?</a></li>
<li><a
href="https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.overcomingbias.com%252Fp%252Fto-imagine-ai-imagine-no-ai.jpg"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.overcomingbias.com%2Fp%2Fto-imagine-ai-imagine-no-ai.jpg" />To
Imagine AI, Imagine No AI</a></li>
<li><a href="https://m.youtube.com/watch?v=9XuVn6nljCM"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fm.youtube.com%252Fwatch%253Fv%253D9XuVn6nljCM.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fm.youtube.com%2Fwatch%3Fv%3D9XuVn6nljCM.png" />Zvi
Mowshowitz &amp; Robin Hanson Discuss AI Risk</a></li>
<li><a
href="https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.lesswrong.com%252Ftag%252Fthe-hanson-yudkowsky-ai-foom-debate.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.lesswrong.com%2Ftag%2Fthe-hanson-yudkowsky-ai-foom-debate.ico" />The
Hanson-Yudkowsky AI-Foom Debate</a></li>
<li><a
href="https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.lesswrong.com%252Fposts%252FgGSvwd62TJAxxhcGh%252Fyudkowsky-vs-hanson-on-foom-whose-predictions-were-better.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgGSvwd62TJAxxhcGh%2Fyudkowsky-vs-hanson-on-foom-whose-predictions-were-better.ico" />Yudkowsky
vs Hanson on FOOM: Whose Predictions Were Better?</a></li>
</ul></div>
    </main>

    <footer>
      <div class="title">
        <div class="root-title">
          <a href="/">rybl.net</a>
        </div>

        <img class="website-icon" src="/image/rybl-small.png" />

        <div class="local-title">Aligned Intelligence and Moral Progress</div>
      </div>
      <div class="menu">
        <div class="item">
          <a href="/">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="24"
              height="24"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="icon lucide lucide-library-icon lucide-library"
            >
              <path d="m16 6 4 14" />
              <path d="M12 6v14" />
              <path d="M8 8v12" />
              <path d="M4 4v16" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/about.html">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="24"
              height="24"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="icon lucide lucide-info-icon lucide-info"
            >
              <circle cx="12" cy="12" r="10" />
              <path d="M12 16v-4" />
              <path d="M12 8h.01" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/profile.html">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="lucide lucide-globe-icon lucide-globe"
            >
              <circle cx="12" cy="12" r="10" />
              <path d="M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20" />
              <path d="M2 12h20" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/graph.html">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="24"
              height="24"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="lucide lucide-orbit-icon lucide-orbit"
            >
              <path d="M20.341 6.484A10 10 0 0 1 10.266 21.85" />
              <path d="M3.659 17.516A10 10 0 0 1 13.74 2.152" />
              <circle cx="12" cy="12" r="3" />
              <circle cx="19" cy="5" r="2" />
              <circle cx="5" cy="19" r="2" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/signature.html"
            ><svg
              xmlns="http://www.w3.org/2000/svg"
              width="24"
              height="24"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="lucide lucide-fingerprint-icon lucide-fingerprint"
            >
              <path d="M12 10a2 2 0 0 0-2 2c0 1.02-.1 2.51-.26 4" />
              <path d="M14 13.12c0 2.38 0 6.38-1 8.88" />
              <path d="M17.29 21.02c.12-.6.43-2.3.5-3.02" />
              <path d="M2 12a10 10 0 0 1 18-6" />
              <path d="M2 16h.01" />
              <path d="M21.8 16c.2-2 .131-5.354 0-6" />
              <path d="M5 19.5C5.5 18 6 15 6 12a6 6 0 0 1 .34-2" />
              <path d="M8.65 22c.21-.66.45-1.32.57-2" />
              <path d="M9 6.8a6 6 0 0 1 9 5.2v2" /></svg
          ></a>
        </div>
      </div></footer>  </body>
</html>
