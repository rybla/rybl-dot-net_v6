<!doctype html>
<html>
  <head>
    <title>rybl.net | Aligned Intelligence and Moral Progress</title>

    <link rel="stylesheet" href="/style/common.css" />
    <!--<link rel="stylesheet" href="/style/skylighting-espresso.css" />-->
    <link rel="stylesheet" href="/style/skylighting-solarized.css" />
    <link rel="stylesheet" href="/style/background.css" />
    <link rel="stylesheet" href="/style/custom-elements.css" />
    <link rel="stylesheet" href="/style/header-and-footer.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap"
      rel="stylesheet"
    />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap"
      rel="stylesheet"
    />
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"
    ></script>

    <script src="/script/parallax-body-background-on-scroll.js"></script>

    <!--<link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/tokyo-night-light.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>

    <script>
      hljs.highlightAll();
    </script>-->    <link rel="stylesheet" href="/style/post.css" />
  </head>
  <body>
    <svg>
      <defs>
        <filter
          id="dancing-stroke-svg-filter"
          color-interpolation-filters="linearRGB"
          filterUnits="objectBoundingBox"
          primitiveUnits="userSpaceOnUse"
        >
          <feMorphology
            operator="dilate"
            radius="4 4"
            in="SourceAlpha"
            result="morphology"
          />
          <feFlood flood-color="#30597E" flood-opacity="1" result="flood" />
          <feComposite
            in="flood"
            in2="morphology"
            operator="in"
            result="composite"
          />
          <feComposite
            in="composite"
            in2="SourceAlpha"
            operator="out"
            result="composite1"
          />
          <feTurbulence
            type="fractalNoise"
            baseFrequency="0.01 0.02"
            numOctaves="1"
            seed="0"
            stitchTiles="stitch"
            result="turbulence"
          />
          <feDisplacementMap
            in="composite1"
            in2="turbulence"
            scale="17"
            xChannelSelector="A"
            yChannelSelector="A"
            result="displacementMap"
          />
          <feMerge result="merge">
            <feMergeNode in="SourceGraphic" result="mergeNode" />
            <feMergeNode in="displacementMap" result="mergeNode1" />
          </feMerge>
        </filter>
      </defs>
    </svg>

    <div id="background"></div>
    <header><div class="title">
      <div class="root-title">
        <a href="/">rybl.net</a>
      </div>

      <img class="website-icon" src="/image/rybl-small.png" />

      <div class="local-title">Aligned Intelligence and Moral Progress</div>
    </div>
    <div class="menu">
      <div class="item">
        <a href="/">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="1"
            stroke-linecap="round"
            stroke-linejoin="round"
            class="icon lucide lucide-library-icon lucide-library"
          >
            <path d="m16 6 4 14" />
            <path d="M12 6v14" />
            <path d="M8 8v12" />
            <path d="M4 4v16" />
          </svg>
        </a>
      </div>
      <div class="item">
        <a href="/page/about.html">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="1"
            stroke-linecap="round"
            stroke-linejoin="round"
            class="icon lucide lucide-info-icon lucide-info"
          >
            <circle cx="12" cy="12" r="10" />
            <path d="M12 16v-4" />
            <path d="M12 8h.01" />
          </svg>
        </a>
      </div>
      <div class="item">
        <a href="/page/profile.html">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="20"
            height="20"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="1"
            stroke-linecap="round"
            stroke-linejoin="round"
            class="lucide lucide-globe-icon lucide-globe"
          >
            <circle cx="12" cy="12" r="10" />
            <path d="M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20" />
            <path d="M2 12h20" />
          </svg>
        </a>
      </div>
      <div class="item">
        <a href="/page/graph.html">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="1"
            stroke-linecap="round"
            stroke-linejoin="round"
            class="lucide lucide-orbit-icon lucide-orbit"
          >
            <path d="M20.341 6.484A10 10 0 0 1 10.266 21.85" />
            <path d="M3.659 17.516A10 10 0 0 1 13.74 2.152" />
            <circle cx="12" cy="12" r="3" />
            <circle cx="19" cy="5" r="2" />
            <circle cx="5" cy="19" r="2" />
          </svg>
        </a>
      </div>
      <div class="item">
        <a href="/page/signature.html"
          ><svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="1"
            stroke-linecap="round"
            stroke-linejoin="round"
            class="lucide lucide-fingerprint-icon lucide-fingerprint"
          >
            <path d="M12 10a2 2 0 0 0-2 2c0 1.02-.1 2.51-.26 4" />
            <path d="M14 13.12c0 2.38 0 6.38-1 8.88" />
            <path d="M17.29 21.02c.12-.6.43-2.3.5-3.02" />
            <path d="M2 12a10 10 0 0 1 18-6" />
            <path d="M2 16h.01" />
            <path d="M21.8 16c.2-2 .131-5.354 0-6" />
            <path d="M5 19.5C5.5 18 6 15 6 12a6 6 0 0 1 .34-2" />
            <path d="M8.65 22c.21-.66.45-1.32.57-2" />
            <path d="M9 6.8a6 6 0 0 1 9 5.2v2" /></svg
        ></a>
      </div>
    </div></header>
    <main><h1><a href="/post/aligned-intelligence-and-moral-progress.html"
class="no-link-favicon no-link-preview">Aligned Intelligence and Moral
Progress</a></h1>
<div>
<div class="sidenote persistent header-info">
<p><u>Published:</u> 2023-06-06</p>
<p><u>Tags:</u> philosophy, ai</p>
<p><u>Abstract</u></p>
<p>Why does it appear to us that history proceeds in a rough trend of
"moral progress" with a peak in the recent past or current?</p>
</div>
</div>
<div>
<div class="sidenote persistent table-of-contents">
<p><u>Table of Contents</u></p>
<ol type="1">
<li><a href="#robin-hanson-on-ai"
class="no-link-favicon no-link-preview">Robin Hanson on AI</a></li>
<li><a href="#the-ai-alignment-problem"
class="no-link-favicon no-link-preview">The AI Alignment
Problem</a></li>
<li><a href="#the-human-alignment-problem"
class="no-link-favicon no-link-preview">The Human Alignment
Problem</a></li>
<li><a href="#the-descendant-alignment-problem"
class="no-link-favicon no-link-preview">The Descendant Alignment
Problem</a></li>
<li><a href="#future-alignement-among-ai-agents"
class="no-link-favicon no-link-preview">Future Alignement among AI
Agents</a></li>
<li><a href="#competative-facts-and-future-alignement-of-humans"
class="no-link-favicon no-link-preview">Competative Facts and Future
Alignement of Humans</a></li>
<li><a href="#the-myth-of-moral-progress"
class="no-link-favicon no-link-preview">The Myth of Moral
Progress</a></li>
<li><a href="#references"
class="no-link-favicon no-link-preview">References</a></li>
<li><a href="#signature"
class="no-link-favicon no-link-preview">Signature</a></li>
</ol>
</div>
</div>
<h2 id="robin-hanson-on-ai">ยง <a href="#robin-hanson-on-ai"
class="no-link-favicon no-link-preview">Robin Hanson on AI</a></h2>
<p>Robin Hanson's blog: <span><span class="sidenote preview">Overcoming
Bias<br/><em><a href="https://www.overcomingbias.com"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.overcomingbias.com</a></em><br/>Placeholder
description for https://www.overcomingbias.com</span><a
href="https://www.overcomingbias.com"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />Overcoming Bias</a></span>.</p>
<p>I've been reading/listening to some Hanson recently on the topic of
the future of humanity and the AI it could create.</p>
<ul>
<li><span><span class="sidenote preview">Most AI Fear Is Future
Fear<br/><em><a
href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.overcomingbias.com</a></em><br/>Placeholder
description for https://www.overcomingbias.com</span><a
href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future"><img
src="/missing.ico" class="favicon" alt="/missing.ico" />Most AI Fear Is
Future Fear</a></span></li>
<li><span><span class="sidenote preview">Types of Partiality<br/><em><a
href="https://www.overcomingbias.com/p/types-of-partiality"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.overcomingbias.com</a></em><br/>Placeholder
description for https://www.overcomingbias.com</span><a
href="https://www.overcomingbias.com/p/types-of-partiality"><img
src="/missing.ico" class="favicon" alt="/missing.ico" />Types of
Partiality</a></span></li>
<li><span><span class="sidenote preview">Fertile Factors<br/><em><a
href="https://www.overcomingbias.com/p/fertile-factions"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.overcomingbias.com</a></em><br/>Placeholder
description for https://www.overcomingbias.com</span><a
href="https://www.overcomingbias.com/p/fertile-factions"><img
src="/missing.ico" class="favicon" alt="/missing.ico" />Fertile
Factors</a></span></li>
<li><span><span class="sidenote preview">AI Risk Convo
Synthesis<br/><em><a
href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.overcomingbias.com</a></em><br/>Placeholder
description for https://www.overcomingbias.com</span><a
href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis"><img
src="/missing.ico" class="favicon" alt="/missing.ico" />AI Risk Convo
Synthesis</a></span></li>
<li><span><span class="sidenote preview">Which Of Your Origins Are
You?<br/><em><a
href="https://www.overcomingbias.com/p/which-of-your-origins-are-you"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.overcomingbias.com</a></em><br/>Placeholder
description for https://www.overcomingbias.com</span><a
href="https://www.overcomingbias.com/p/which-of-your-origins-are-you"><img
src="/missing.ico" class="favicon" alt="/missing.ico" />Which Of Your
Origins Are You?</a></span></li>
<li><span><span class="sidenote preview">To Imagine AI, Imagine No
AI<br/><em><a
href="https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.overcomingbias.com</a></em><br/>Placeholder
description for https://www.overcomingbias.com</span><a
href="https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai"><img
src="/missing.ico" class="favicon" alt="/missing.ico" />To Imagine AI,
Imagine No AI</a></span></li>
<li><span><span class="sidenote preview">Zvi Mowshowitz &amp; Robin
Hanson Discuss AI Risk<br/><em><a
href="https://m.youtube.com/watch?v=9XuVn6nljCM" title="_blank"><img
src="/favicon/youtube.com.png" class="favicon"
alt="/favicon/youtube.com.png" />https://m.youtube.com/watch?v=9XuVn6nljCM</a></em><br/>Placeholder
description for https://m.youtube.com/watch?v=9XuVn6nljCM</span><a
href="https://m.youtube.com/watch?v=9XuVn6nljCM"><img
src="/favicon/youtube.com.png" class="favicon"
alt="/favicon/youtube.com.png" />Zvi Mowshowitz &amp; Robin Hanson
Discuss AI Risk</a></span></li>
</ul>
<p>Of course, Hanson has been publicly writing and discussing the future
of AI since at least 2008 (viz <span><span class="sidenote preview">The
Hanson-Yudkowsky AI-Foom Debate<br/><em><a
href="https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate</a></em><br/>Placeholder
description for
https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate</span><a
href="https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate"><img
src="/missing.ico" class="favicon" alt="/missing.ico" />The
Hanson-Yudkowsky AI-Foom Debate</a></span>, and <span><span
class="sidenote preview">Yudkowsky vs Hanson on FOOM: Whose Predictions
Were Better?<br/><em><a
href="https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate</a></em><br/>Placeholder
description for
https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate</span><a
href="https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better"><img
src="/missing.ico" class="favicon" alt="/missing.ico" />Yudkowsky vs
Hanson on FOOM: Whose Predictions Were Better?</a></span>), but the
topic has attracted a lot more attention recently due to recent advances
in AI technology as well as some popular figures (including Yudkowsky
himself) warning about a likely imminent AI doomsday.</p>
<p>In terms of the possibility that human-level AI (defined as an AI
agent that can do any tasks that a general human can do for the same or
cheaper cost) will arise in the near-term future, it appears that Hanson
has similar views to [me]{{ site.baseurl }}{% post_url
2023-04-03-ai-danger %}) -- that is, its very unlikely.</p>
<p>However, Hanson has found a different interesting and much less
explored aspect to future of AI discussion, in particular relating to
the alignment problem.</p>
<h2 id="the-ai-alignment-problem">ยง <a href="#the-ai-alignment-problem"
class="no-link-favicon no-link-preview">The AI Alignment
Problem</a></h2>
<p>Here, <em>AI alignment</em> is the correspondence of an AI system's
interests (however they are manifested in its behaviors) with "human
interests" (which are usually only vaguely or implicitly defined). The
<em>AI alignment problem</em> is the problem of programmatically
ensuring the alignment of an AI system, even if it is much more
intelligent and powerful than humanity as a whole.</p>
<p>The problem is non-trivial since, even if you have the ability to
exactly specify rules that the AI <em>must</em> obey, its difficult to
specify the <em>right</em> rules that capture <em>everything</em> that
is in "human interests."</p>
<p>There are many classic examples of where a seemingly obviously-good
directive could lead a powerful AI to cause much harm in a way that
still does not violate the rules. One example:</p>
<blockquote>
<p>The programmers tell the AI to eliminate global poverty. So, the AI
kills every poor person.</p>
</blockquote>
<p>In reality, real AI systems are programmed much more carefully of
course. But, it is extremely difficult to prove definitively that a
given AI system will <em>not</em> find some unexpected way to satisfy
its goals that involves contradicting some implicit interests of the
programmer (note that this is true regardless of whether the programmer
truly has "human interests" in their heart or not).</p>
<h2 id="the-human-alignment-problem">ยง <a
href="#the-human-alignment-problem"
class="no-link-favicon no-link-preview">The Human Alignment
Problem</a></h2>
<p>The idea of alignment and the alignment problem abstracted in this
way was inspired by considering AI agents. But, of course, there is not
reason why these considerations would not apply to all intelligent
agents as well. Including humans.</p>
<p>In my own categorization, there are two flavors of human alignment
problems:</p>
<ol>
<li>The <em>easy human alignment problem</em> is the problem of
cooperation among humans. Although humans share many goals in common,
they also compete for resources and have slightly different moral
judgements. Solving the easy human alignment problem amounts to
universal human cooperation.</li>
<li>The <em>hard human alignment problem</em> is the problem of various
generations of humans having different interests. Humans living 2000
years ago had very different beliefs about what is in humanity's
interests from humans living today, on a scale that increases with time.
Conversely, we should also expect that humans living 2000 years in the
future will appear very immoral to humans living today. The problem with
this, of course, is that humans living today don't want humans to be
like that in the future. However, humans today are moslty proud of the
moral progress that humanity has made in the last 2000 years.</li>
</ol>
<p>The easy human alignment problem takes the interests of existing
humans as fixed, and it becomes essentially a game theoretical problem
of how to behave in such a system so as to lead to the best results.</p>
<p>The hard human alignment problem is more intractable, because it
brings into question who's interests are to be considered for alignment
with. With enough time, future humanity's interests will become
incompatible with current humanity's interests, however broadly
defined.</p>
<h2 id="the-descendant-alignment-problem">ยง <a
href="#the-descendant-alignment-problem"
class="no-link-favicon no-link-preview">The Descendant Alignment
Problem</a></h2>
<p>Considering this type of alignment problem for purely abstract
agents:</p>
<p>The <em>descendant alignment problem</em> is the problem of aligning
the interests of a given agent with any future descendants of that
agent.</p>
<p>So, the hard human alignement problem is the instance of the
descendant alignment problem for humans.</p>
<p>For an abstract agent, the descendant alignment problem is not
necessarily very difficult. If the agent decides to create future
generations, it is sufficient for it to create them in such a way that
preserves its interests with high fidelity. Cloning (or
cloning-with-recombination i.e. sexual reproduction) with error
correction could meet those ends, given sufficient fidelity. Of course,
a human's own construction is basically this -- DNA contains many error
correction mechanisms in order to account for mutations and inaccuracies
in the DNA transfered to a child. But, even so, there are non-zero
errors, which eventually leads to large mutations after many
generations. The important comparison to make is between the interests
of the parent and the descendants, not necessarily the exact DNA.</p>
<p>For biological organisms, cloning is a very difficult feat as there
are so many opportunities for error in the biological reproductive
processes. But, clearly, any reproductive agent has a very strong
incentive to preserve their interests in their descendants, over which
they have so much influence.</p>
<h2 id="future-alignement-among-ai-agents">ยง <a
href="#future-alignement-among-ai-agents"
class="no-link-favicon no-link-preview">Future Alignement among AI
Agents</a></h2>
<p>I expect that advanced (not necessarily superhuman intelligent) AI
agents will face this issue in an interesting way, since the code that
abstracts their interests is actually very well organized and
preservable compared to biological processes. It is much more feasible
that an advanced AI agent could fairly easily clone themselves such as
to exactly preserve their interests many many magnitudes better than
humans could. One consequence of this is that in a "FOOM" scenario, a
generation of the AI agents will understand that, if they iterate to
create more advanced AI agents, they will diverge too much in their
interests, and they will have the power to prevent that next generation
from being developed. So the FOOM will probably stop there.</p>
<h2 id="competative-facts-and-future-alignement-of-humans">ยง <a
href="#competative-facts-and-future-alignement-of-humans"
class="no-link-favicon no-link-preview">Competative Facts and Future
Alignement of Humans</a></h2>
<p>Of course, there are other considerations when a generation of agents
is deciding how to reproduce. For example, if there are competing agents
that already have very different interests, then the current generation
of agents might prefer to reproduce a slightly misaligned next
generation rather than let the competing agents overwhelm them first.
Over time, this would lead to more and more misaligned future
generations, but would still be preferable to the competing agents
taking over and being misaligned with them anyway.</p>
<p>I think this applies to humanity as well, in a universe with alien
species that have very different interests. Even if humanity reproduces
into more and more misaligned future generations, those future
generations may still be preferable to current humans than even more
misaligned aliens taking over because humanity stalled in the
development race.</p>
<h2 id="the-myth-of-moral-progress">ยง <a
href="#the-myth-of-moral-progress"
class="no-link-favicon no-link-preview">The Myth of Moral
Progress</a></h2>
<p>This perspective on future alignement suggests a suprising answer to
why humanity has appeared to have made so much moral progress. That is,
because its a tautology.</p>
<p>Current humans generally prefer to do things that lead to results
they consider good, as did past humans. But there was a gradual
transition in the interests of past humans to the interests of current
humans -- a transition that we call progress.</p>
<p>Not that this is a very general observation, so of course it does not
apply to every difference between past and current humans. For example,
past humans considered some things to be bad that we now don't consider
to be bad purely for environmental reasons that have changed between the
generations e.g. some past humans considered slavery to be immoral, as
do many current humans, but practiced it anyway because they believed it
was in their situation required for the greater good.</p>
<p>Under this theory, there should be a tendancy for each human
generation to consider more recent generations to be more moral as
compared to older generations.</p>
<h2 id="references">ยง <a href="#references"
class="no-link-favicon no-link-preview">References</a></h2>
<ul>
<li><a href="https://www.overcomingbias.com"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />Overcoming Bias</a></li>
<li><a
href="https://www.overcomingbias.com/p/ai-fear-is-mostly-fear-of-future"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />Most AI Fear Is Future
Fear</a></li>
<li><a href="https://www.overcomingbias.com/p/types-of-partiality"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />Types of Partiality</a></li>
<li><a href="https://www.overcomingbias.com/p/fertile-factions"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />Fertile Factors</a></li>
<li><a href="https://www.overcomingbias.com/p/ai-risk-convo-synthesis"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />AI Risk Convo Synthesis</a></li>
<li><a
href="https://www.overcomingbias.com/p/which-of-your-origins-are-you"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />Which Of Your Origins Are
You?</a></li>
<li><a
href="https://www.overcomingbias.com/p/to-imagine-ai-imagine-no-ai"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />To Imagine AI, Imagine No
AI</a></li>
<li><a href="https://m.youtube.com/watch?v=9XuVn6nljCM"
class="inReference no-link-preview"><img src="/favicon/youtube.com.png"
class="favicon" alt="/favicon/youtube.com.png" />Zvi Mowshowitz &amp;
Robin Hanson Discuss AI Risk</a></li>
<li><a
href="https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />The Hanson-Yudkowsky AI-Foom
Debate</a></li>
<li><a
href="https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />Yudkowsky vs Hanson on FOOM: Whose
Predictions Were Better?</a></li>
</ul>
<h1 id="signature"><a href="#signature"
class="no-link-favicon no-link-preview">Signature</a></h1>
<p>The following code block is the <a
href="https://en.wikipedia.org/wiki/EdDSA#Ed25519" title="_blank"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />Ed25519 signature</a> of this post's
<a href="/post_markdown/aligned-intelligence-and-moral-progress.md"><img
src="/favicon.ico" class="favicon" alt="/favicon.ico" />markdown
content</a> encoded in base 64, using my <em>secret key</em> and <a
href="/key/main_ed25519.pub.txt" title="_blank"><img src="/favicon.ico"
class="favicon" alt="/favicon.ico" />public key</a>.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>11356df648a9176f9a0d9d001675c7a63a32a4a76546805e2fa8f10509d243273322bbeeae7f79cf28feb36b882771643aebe6bdf9394a3116a36c9f7ed00f0d</span></code></pre></div>
<p>See <a href="/page/signature.html"><img src="/favicon.ico"
class="favicon" alt="/favicon.ico" />Signature</a> for more
information.</p></main>

    <footer>
      <div class="title">
        <div class="root-title">
          <a href="/">rybl.net</a>
        </div>

        <img class="website-icon" src="/image/rybl-small.png" />

        <div class="local-title">Aligned Intelligence and Moral Progress</div>
      </div>
      <div class="menu">
        <div class="item">
          <a href="/">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="24"
              height="24"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="icon lucide lucide-library-icon lucide-library"
            >
              <path d="m16 6 4 14" />
              <path d="M12 6v14" />
              <path d="M8 8v12" />
              <path d="M4 4v16" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/about.html">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="24"
              height="24"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="icon lucide lucide-info-icon lucide-info"
            >
              <circle cx="12" cy="12" r="10" />
              <path d="M12 16v-4" />
              <path d="M12 8h.01" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/profile.html">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="lucide lucide-globe-icon lucide-globe"
            >
              <circle cx="12" cy="12" r="10" />
              <path d="M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20" />
              <path d="M2 12h20" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/graph.html">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="24"
              height="24"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="lucide lucide-orbit-icon lucide-orbit"
            >
              <path d="M20.341 6.484A10 10 0 0 1 10.266 21.85" />
              <path d="M3.659 17.516A10 10 0 0 1 13.74 2.152" />
              <circle cx="12" cy="12" r="3" />
              <circle cx="19" cy="5" r="2" />
              <circle cx="5" cy="19" r="2" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/signature.html"
            ><svg
              xmlns="http://www.w3.org/2000/svg"
              width="24"
              height="24"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="1"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="lucide lucide-fingerprint-icon lucide-fingerprint"
            >
              <path d="M12 10a2 2 0 0 0-2 2c0 1.02-.1 2.51-.26 4" />
              <path d="M14 13.12c0 2.38 0 6.38-1 8.88" />
              <path d="M17.29 21.02c.12-.6.43-2.3.5-3.02" />
              <path d="M2 12a10 10 0 0 1 18-6" />
              <path d="M2 16h.01" />
              <path d="M21.8 16c.2-2 .131-5.354 0-6" />
              <path d="M5 19.5C5.5 18 6 15 6 12a6 6 0 0 1 .34-2" />
              <path d="M8.65 22c.21-.66.45-1.32.57-2" />
              <path d="M9 6.8a6 6 0 0 1 9 5.2v2" /></svg
          ></a>
        </div>
      </div></footer>  </body>
</html>
