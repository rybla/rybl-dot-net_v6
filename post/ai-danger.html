<!doctype html>
<html>
  <head>
    <title>rybl.net | AI Danger</title>

    <link rel="stylesheet" href="/style/common.css" />
    <!--<link rel="stylesheet" href="/style/skylighting-espresso.css" />-->
    <link rel="stylesheet" href="/style/skylighting-solarized.css" />
    <link rel="stylesheet" href="/style/background.css" />
    <link rel="stylesheet" href="/style/custom-elements.css" />
    <link rel="stylesheet" href="/style/header-and-footer.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap"
      rel="stylesheet"
    />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap"
      rel="stylesheet"
    />
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"
    ></script>

    <script src="/script/parallax-body-background-on-scroll.js"></script>

    <!--<link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/tokyo-night-light.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>

    <script>
      hljs.highlightAll();
    </script>-->    <link rel="stylesheet" href="/style/post.css" />
  </head>
  <body>
    <svg>
      <defs>
        <filter
          id="dancing-stroke-svg-filter"
          color-interpolation-filters="linearRGB"
          filterUnits="objectBoundingBox"
          primitiveUnits="userSpaceOnUse"
        >
          <feMorphology
            operator="dilate"
            radius="4 4"
            in="SourceAlpha"
            result="morphology"
          />
          <feFlood flood-color="#30597E" flood-opacity="1" result="flood" />
          <feComposite
            in="flood"
            in2="morphology"
            operator="in"
            result="composite"
          />
          <feComposite
            in="composite"
            in2="SourceAlpha"
            operator="out"
            result="composite1"
          />
          <feTurbulence
            type="fractalNoise"
            baseFrequency="0.01 0.02"
            numOctaves="1"
            seed="0"
            stitchTiles="stitch"
            result="turbulence"
          />
          <feDisplacementMap
            in="composite1"
            in2="turbulence"
            scale="17"
            xChannelSelector="A"
            yChannelSelector="A"
            result="displacementMap"
          />
          <feMerge result="merge">
            <feMergeNode in="SourceGraphic" result="mergeNode" />
            <feMergeNode in="displacementMap" result="mergeNode1" />
          </feMerge>
        </filter>
      </defs>
    </svg>

    <div id="background"></div>
    <header><div class="title">
      <div class="root-title">
        <a href="/">rybl.net</a>
      </div>

      <img class="website-icon" src="/image/rybl-small.png" />

      <div class="local-title">AI Danger</div>
    </div>
    <div class="menu">
      <div class="item">
        <a href="/">
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
            viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
            stroke-linecap="round" stroke-linejoin="round"
            class="lucide lucide-circle-slash-icon lucide-circle-slash">
            <circle cx="12" cy="12" r="10" />
            <line x1="9" x2="15" y1="15" y2="9" />
          </svg>
        </a>
      </div>
      <div class="item">
        <a href="/page/library.html">
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
            viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
            stroke-linecap="round" stroke-linejoin="round"
            class="icon lucide lucide-library-icon lucide-library">
            <path d="m16 6 4 14" />
            <path d="M12 6v14" />
            <path d="M8 8v12" />
            <path d="M4 4v16" />
          </svg>
        </a>
      </div>
      <div class="item">
        <a href="/page/about.html">
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
            viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
            stroke-linecap="round" stroke-linejoin="round"
            class="icon lucide lucide-info-icon lucide-info">
            <circle cx="12" cy="12" r="10" />
            <path d="M12 16v-4" />
            <path d="M12 8h.01" />
          </svg>
        </a>
      </div>
      <div class="item">
        <a href="/page/profile.html">
          <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20"
            viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
            stroke-linecap="round" stroke-linejoin="round"
            class="lucide lucide-globe-icon lucide-globe">
            <circle cx="12" cy="12" r="10" />
            <path d="M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20" />
            <path d="M2 12h20" />
          </svg>
        </a>
      </div>
      <div class="item">
        <a href="/page/references-graph.html">
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
            viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
            stroke-linecap="round" stroke-linejoin="round"
            class="lucide lucide-orbit-icon lucide-orbit">
            <path d="M20.341 6.484A10 10 0 0 1 10.266 21.85" />
            <path d="M3.659 17.516A10 10 0 0 1 13.74 2.152" />
            <circle cx="12" cy="12" r="3" />
            <circle cx="19" cy="5" r="2" />
            <circle cx="5" cy="19" r="2" />
          </svg>
        </a>
      </div>
      <div class="item">
        <a href="/page/signature.html"><svg xmlns="http://www.w3.org/2000/svg"
            width="24" height="24" viewBox="0 0 24 24" fill="none"
            stroke="currentColor" stroke-width="1" stroke-linecap="round"
            stroke-linejoin="round"
            class="lucide lucide-fingerprint-icon lucide-fingerprint">
            <path d="M12 10a2 2 0 0 0-2 2c0 1.02-.1 2.51-.26 4" />
            <path d="M14 13.12c0 2.38 0 6.38-1 8.88" />
            <path d="M17.29 21.02c.12-.6.43-2.3.5-3.02" />
            <path d="M2 12a10 10 0 0 1 18-6" />
            <path d="M2 16h.01" />
            <path d="M21.8 16c.2-2 .131-5.354 0-6" />
            <path d="M5 19.5C5.5 18 6 15 6 12a6 6 0 0 1 .34-2" />
            <path d="M8.65 22c.21-.66.45-1.32.57-2" />
            <path d="M9 6.8a6 6 0 0 1 9 5.2v2" />
          </svg></a>
      </div>
    </div></header>
    <main><h1><a href="/post/ai-danger.html"
class="no-link-favicon no-link-preview">AI Danger</a></h1>
<div>
<div class="sidenote persistent header-info">
<p><u>Published:</u> 2023-04-03</p>
<p><u>Tags:</u> ai</p>
<p><u>Abstract</u></p>
<p>My thoughts on the dangers of AI technology.</p>
</div>
</div>
<div>
<div class="sidenote persistent table-of-contents">
<p><u>Table of Contents</u></p>
<ol type="1">
<li><a href="#what-are-the-dangers-of-ai"
class="no-link-favicon no-link-preview">What are the Dangers of
AI?</a></li>
<li><a href="#how-likely-is-ai-doomsday"
class="no-link-favicon no-link-preview">How Likely Is AI Doomsday?</a>
<ol type="1">
<li><a href="#important-reservations"
class="no-link-favicon no-link-preview">Important Reservations</a></li>
</ol></li>
<li><a href="#how-likely-is-ai-doomsday-soon"
class="no-link-favicon no-link-preview">How Likely Is AI Doomsday,
Soon?</a>
<ol type="1">
<li><a href="#timeline-to-hlmi"
class="no-link-favicon no-link-preview">Timeline to HLMI</a></li>
<li><a
href="#most-ai-researchers-expect-that-superintelligent-ai-will-not-be-extremely-bad"
class="no-link-favicon no-link-preview">Most AI Researchers Expect that
Superintelligent AI will <em>not</em> be Extremely Bad</a></li>
<li><a href="#my-current-position-on-imminent-ai-doomsday"
class="no-link-favicon no-link-preview">My Current Position on Imminent
AI Doomsday</a></li>
<li><a href="#what-i-would-consider-a-major-shortcoming-in-my-reasoning"
class="no-link-favicon no-link-preview">What I Would Consider a Major
Shortcoming in My Reasoning</a></li>
<li><a href="#what-i-would-consider-significant-steps-towards-hlmi"
class="no-link-favicon no-link-preview">What I Would Consider
Significant Steps Towards HLMI</a></li>
</ol></li>
<li><a
href="#how-hard-is-it-to-avoid-ai-doomsday-aka-the-alignment-problem"
class="no-link-favicon no-link-preview">How Hard Is It To Avoid AI
Doomsday? (aka The Alignment Problem)</a></li>
<li><a href="#more-resource"
class="no-link-favicon no-link-preview">More Resource</a></li>
<li><a href="#references"
class="no-link-favicon no-link-preview">References</a></li>
<li><a href="#signature"
class="no-link-favicon no-link-preview">Signature</a></li>
</ol>
</div>
</div>
<div class="definition">
<p>The <strong>AI Doomsday</strong> scenario is a possible future
scenario where an AI system is developed that overcomes all attempts at
control and very soon after the system causes the extinction of the
human species. This definition abstracts away any particular prediction
of <em>when</em> this scenario will happen and <em>how</em> specifically
it will play out.</p>
</div>
<p><img src="/asset/ai-danger/ai-danger-by-dalle-2.png"
alt="illustration of &quot;AI danger&quot; by Dalle 2" /></p>
<h2 id="what-are-the-dangers-of-ai">§ <a
href="#what-are-the-dangers-of-ai"
class="no-link-favicon no-link-preview">What are the Dangers of
AI?</a></h2>
<p>AI has been a hot topic recently due to some impressive <span><span
class="sidenote preview">recent advancements<br/><em><a
href="https://lifearchitect.ai/timeline" title="_blank"><img
src="/favicon/lifearchitect.ai.ico" class="favicon"
alt="/favicon/lifearchitect.ai.ico" />https://lifearchitect.ai/timeline</a></em><br/>Placeholder
description for https://lifearchitect.ai/timeline</span><a
href="https://lifearchitect.ai/timeline"><img
src="/favicon/lifearchitect.ai.ico" class="favicon"
alt="/favicon/lifearchitect.ai.ico" />recent advancements</a></span>. In
particular, the chatbot web app <em>ChatGPT</em> by <span><span
class="sidenote preview">Open AI<br/><em><a href="http://openai.com/"
title="_blank"><img src="/favicon/openai.com.svg" class="favicon"
alt="/favicon/openai.com.svg" />OpenAI</a></em><br/>OpenAI is an
American AI research and deployment non-profit company that was founded
in 2015. The company is known for creating ChatGPT.</span><a
href="http://openai.com/"><img src="/favicon/openai.com.svg"
class="favicon" alt="/favicon/openai.com.svg" />Open AI</a></span> has
given the public a first view of a new kind of AI technology that can be
integrated into one's everyday life and demonstrates surprisingly
human-like capabilities.</p>
<p>On the internet I've seen a large variety of responses to the new and
quickly improving AI technology.</p>
<p>Many people are interested in how to use the technology to build
newly-possible tools and products.</p>
<p>Many people are concerned with the dangers brought by this new
technology that society at large is ill-prepared for. Overall, some main
(not entirely independent) concerns I've noticed are: - AI will soon be
able to perform many jobs more efficiently (in terms of meeting a
threshold quality/cost for employers) than many humans, which will lead
to mass unemployment. - AI will enable more proliferation of
misinformation, which will lead to further breakdown of social/political
institutions/relations. - AI will lead to a massive increase in
AI-generated content (writing, digital art, etc) that is hard to
distinguish from humans, which will decrease the value of
human-generated content and promote bullshit. - An AI system will be
developed that overcomes all attempts at control and soon after the
system causes the extinction of the human species (this is the <em>AI
Doomsday</em> scenario)</p>
<p>Many people are in favor of focusing pre-emptively on addressing
these dangers before they fully play out. For example, the letter
<em><span><span class="sidenote preview">Pause Giant AI Experiments: An
Open Letter<br/><em><a
href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"
title="_blank"><img src="/favicon/futureoflife.org.png" class="favicon"
alt="/favicon/futureoflife.org.png" />https://futureoflife.org/open-letter/pause-giant-ai-experiments/</a></em><br/>Placeholder
description for
https://futureoflife.org/open-letter/pause-giant-ai-experiments/</span><a
href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"><img
src="/favicon/futureoflife.org.png" class="favicon"
alt="/favicon/futureoflife.org.png" />Pause Giant AI Experiments: An
Open Letter</a></span></em> (and associated <span><span
class="sidenote preview">FAQ<br/><em><a
href="https://futureoflife.org/ai/faqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments/"
title="_blank"><img src="/favicon/futureoflife.org.png" class="favicon"
alt="/favicon/futureoflife.org.png" />https://futureoflife.org/open-letter/pause-giant-ai-experiments/</a></em><br/>Placeholder
description for
https://futureoflife.org/open-letter/pause-giant-ai-experiments/</span><a
href="https://futureoflife.org/ai/faqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments/"><img
src="/favicon/futureoflife.org.png" class="favicon"
alt="/favicon/futureoflife.org.png" />FAQ</a></span>) from the Future of
Life Institute calls for a voluntary 6-month "pause" on AI development
by the AI industry leaders (in particular, Open AI/Microsoft and Google)
to call attention to and give time for efforts to address the near-term
dangers of AI.</p>
<p>Pausing AI development for even just 6 months is a <em>huge</em> ask.
Companies like Open AI, Microsoft, Google, Anthropic, and Midjourney are
spending billions of USD to develop these technologies and produce
billions of USD of value. The most recent AI developments have already
gained widespread use e.g. Open AI's ChatGPT accumulated at least 100
million users in just 2 months [<span><span
class="sidenote preview">reuters<br/><em><a
href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/"
title="_blank"><img src="/favicon/reuters.com.ico" class="favicon"
alt="/favicon/reuters.com.ico" />https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/</a></em><br/>Placeholder
description for
https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/</span><a
href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/"><img
src="/favicon/reuters.com.ico" class="favicon"
alt="/favicon/reuters.com.ico" />reuters</a></span>].</p>
<p>The letter justifies this huge ask by pointing out the huge dangers
of AI technology, in particular:</p>
<ul>
<li>mass unemployment</li>
<li>mass misinformation</li>
<li>"loss of control of our civilization" (likely AI Doomsday)</li>
</ul>
<p>In addition to the impact of these dangers when they play out, the
probability distribution of various scales of these dangers is also
critical for weighing this justification.</p>
<p>There are surely many other dangers to AI technology, but in this
post, I will focus primarily on AI Doomsday.</p>
<h2 id="how-likely-is-ai-doomsday">§ <a
href="#how-likely-is-ai-doomsday"
class="no-link-favicon no-link-preview">How Likely Is AI
Doomsday?</a></h2>
<div class="definition">
<p><strong>Eventual AI Doomsday</strong> is the prediction that
<strong>AI Doomsday</strong> will happen at some point in the
future.</p>
</div>
<p>In principle, AI Doomsday is very likely to happen. <span><span
class="sidenote preview">Nick Bostrom<br/><em><a
href="https://en.wikipedia.org/wiki/Nick_Bostrom" title="_blank"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />https://en.wikipedia.org/wiki/Escrow</a></em><br/>Placeholder
description for https://en.wikipedia.org/wiki/Escrow</span><a
href="https://en.wikipedia.org/wiki/Nick_Bostrom"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />Nick Bostrom</a></span> popularized
this observation in <span><span
class="sidenote preview">Superintelligence<br/><em><a
href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"
title="_blank"><img src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />https://en.wikipedia.org/wiki/Escrow</a></em><br/>Placeholder
description for https://en.wikipedia.org/wiki/Escrow</span><a
href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />Superintelligence</a></span>, and
<span><span class="sidenote preview">Eliezer Yudkowsky<br/><em><a
href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky"
title="_blank"><img src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />https://en.wikipedia.org/wiki/Escrow</a></em><br/>Placeholder
description for https://en.wikipedia.org/wiki/Escrow</span><a
href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />Eliezer Yudkowsky</a></span> has
spent much of his time rebutting his own and others' suggestions for how
to prevent an advanced AI system that <em>could</em> cause AI Doomsday
from doing so. I find the most convincing argument for AI Doomsday to be
the most abstract version. The argument has two parts:</p>
<ol>
<li><em>A sufficiently advanced AI system can cause AI Doomsday</em>.
Suppose there exists at some point an AI system that is so capable that
if human efforts to maintain control over it fail then it will never be
controllable again by humans after that point. Then it is very likely
that there will be some point after this development that the AI does
escape control in this way, and pursues its own goals. It is vanishingly
unlikely that the system's pursuit of these goals will leave room for
human survival.</li>
<li><em>Such a sufficiently advanced AI is likely to be developed</em>.
Suppose that the advancement of AI technology continues to be
non-negligible. Then at some point, human-level AI systems will be
developed. These systems will be able to aid in the development of more
advanced AI systems. So, it is very likely that the development of AI
technology will continue after that. If this trend continues
non-negligibly, it is likely that eventually, the technology will
develop to a capability that satisfies part 1 of this argument.</li>
</ol>
<p>This argument relies on a few premises: - The advancement of AI
technology continues, however sporadically, until human-level AI is
developed - AI advancement, especially after achieving human-level AI,
will outpace efforts to keep the AI systems under sufficient human
control to prevent part 1 from playing out - Such a capable AI system
beyond human control as in part 1 would pursue goals that would not
include human survival</p>
<p>Of course, it's difficult to directly judge the likelihood of these
premises in the <em>foreseeable</em> future, but over the long term of
humanity's <em>entire</em> future, they seem very likely in principle.
For example, there is a rough upper bound on the difficulty of
developing human-level AI: the possibility of artificially replicating
human intelligence based exactly on biological human intelligence (i.e.
digitally simulating a brain with sufficient detail to yield
intelligence).</p>
<p>I <em>do</em> believe the eventual AI Doomsday prediction is more
likely than not. I am very unsure, so a specific number is not very
useful, but I'd put it at something like a normal distribution of around
80% with a standard deviation of 20%.</p>
<p>TODO: give argument for claim: "It is vanishingly unlikely that the
system's pursuit of these goals will leave room for human survival."</p>
<h3 id="important-reservations">§ <a href="#important-reservations"
class="no-link-favicon no-link-preview">Important Reservations</a></h3>
<p>This is only an argument for an <em>eventual</em> AI Doomsday. It
does not distinguish between an imminent AI Doomsday that happens in 20
years or a long-term AI Doomsday that happens in 20,000 years. And the
argument by itself does not necessarily imply any significant
predictions about the distribution over these timelines.</p>
<p>This argument only makes reference to <em>intelligence</em>, which is
the ability of a system to accomplish general classes of complex goals.
This may turn out to have some relation to <em>consciousness</em>, but
that is a separate topic and the argument doesn't depend on any details
there. And so, the argument does not require any premises about imbuing
an AI system with consciousness.</p>
<h2 id="how-likely-is-ai-doomsday-soon">§ <a
href="#how-likely-is-ai-doomsday-soon"
class="no-link-favicon no-link-preview">How Likely Is AI Doomsday,
Soon?</a></h2>
<div class="definition">
<p><strong>Imminent AI Doomsday</strong> is the prediction that
<strong>AI Doomsday</strong> will happen within the next couple of
decades. Within that timeline, there is a variety of popular probability
distributions.</p>
</div>
<p>I have seen this survey -- <span><span class="sidenote preview">2022
Expert Survey on Progress in AI<br/><em><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
title="_blank"><img src="/favicon/aiimpacts.org.ico" class="favicon"
alt="/favicon/aiimpacts.org.ico" />https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</a></em><br/>Placeholder
description for
https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</span><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"><img
src="/favicon/aiimpacts.org.ico" class="favicon"
alt="/favicon/aiimpacts.org.ico" />2022 Expert Survey on Progress in
AI</a></span> -- often cited to support the claim that most AI
researchers expect artificial general intelligence (AGI) to be developed
in the near future. It's also interesting to compare these results to
<span><span class="sidenote preview">2016 version of this
survey<br/><em><a
href="https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/"
title="_blank"><img src="/favicon/aiimpacts.org.ico" class="favicon"
alt="/favicon/aiimpacts.org.ico" />https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</a></em><br/>Placeholder
description for
https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</span><a
href="https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/"><img
src="/favicon/aiimpacts.org.ico" class="favicon"
alt="/favicon/aiimpacts.org.ico" />2016 version of this
survey</a></span>. Quoting a few main results of the 2022 survey:</p>
<ol>
<li><strong>The aggregate forecast time to a 50% chance of HLMI was 37
years</strong>, i.e. 2059 (not including data from questions about the
conceptually similar Full Automation of Labor, which in 2016 received
much later estimates). This timeline has become about eight years
shorter in the six years since 2016, when the aggregate prediction put a
50% probability at 2061, i.e. 45 years out. Note that these estimates
are conditional on "human scientific activity continu[ing] without major
negative disruption."</li>
<li><strong>The median respondent believes the probability that the
long-run effect of advanced AI on humanity will be "extremely bad (e.g.,
human extinction)" is 5%.</strong> This is the same as it was in 2016
(though Zhang et al 2022 found 2% in a similar but non-identical
question). Many respondents were substantially more concerned: 48% of
respondents gave at least a 10% chance of an extremely bad outcome. But
some were much less concerned: 25% put it at 0%.</li>
<li><strong>The median respondent thinks there is an "about even chance"
that a stated argument for an intelligence explosion is broadly
correct.</strong> 54% of respondents say the likelihood that it is
correct is "about even," "likely," or "very likely" (corresponding to
probability &gt;40%), similar to 51% of respondents in 2016. The median
respondent also believes machine intelligence will probably (60%) be
"vastly better than humans at all professions" within 30 years of HLMI,
and the rate of global technological improvement will probably (80%)
dramatically increase (e.g., by a factor of ten) as a result of machine
intelligence within 30 years of HLMI.</li>
</ol>
<p>(Here, "HLMI" abbreviates "human-level machine intelligence". These
definitions can be hard to justify since there are no well-established
theories of intelligence. The usual conception of AGI includes HLMI i.e.
the "general" in AGI means "more general than (some sort of
reasonably-taken average) human intelligence". I find HLMI a much less
ambiguous term than AGI, so I prefer it, but it's also clear that AGI
has become the popular term of choice for this kind of concept
regardless of its original intentions.)</p>
<p>Imminent AI Doomsday predictions often cite the 37-year average
forecast time to a 50% chance of HLMI. But, note that the probability is
conditional on "human scientific activity continu[ing] without major
negative disruption", which of course I would not expect the surveyed
population to have any special insight into (in contrast to the topic of
AI technological development). Additionally, the response is subject to
selection biases:</p>
<ul>
<li><p>The population is self-selected to be excited about AI technology
since they decided to focus their career working on it, which implies
they rate the probability of achieving one of their main goals --
developing HLMI -- higher due to desirability bias</p></li>
<li><p>The population is self-selected to be less worried about AI
Doomsday since those that are very worried about AI Doomsday would be
less interested in contributing to it. Additionally, AI researchers are
incentivized to underrate the likelihood of AI Doomsday because a higher
perceived likelihood would probably lead to more regulation and other
inhibitions to their primary work and source of income.</p></li>
<li><p>The population is self-selected to rate the timeline to HLMI as
shorter and the likelihood of AI Doomsday as higher due to people with
those beliefs being more interested in a survey asking about those
relatively-niche topics. The survey only got a 17% response rate, which
of course was not a uniformly-random 17%. So, this bias could be quite
significant. The survey's Methods section's description of the surveyed
population:</p></li>
</ul>
<blockquote>
<p>We contacted approximately 4271 researchers who published at the
conferences NeurIPS or ICML in 2021. These people were selected by
taking all of the authors at those conferences and randomly allocating
them between this survey and a survey being run by others. We then
contacted those whose email addresses we could find. We found email
addresses in papers published at those conferences, in other public
data, and in records from our previous survey and Zhang et al 2022. We
received 738 responses, some partial, for a 17% response rate.
Participants who previously participated in the the 2016 ESPAI or Zhang
et al surveys received slightly longer surveys, and received questions
which they had received in past surveys (where random subsets of
questions were given), rather than receiving newly randomized questions.
This was so that they could also be included in a 'matched panel’
survey, in which we contacted all researchers who completed the 2016
ESPAI or Zhang et al surveys, to compare responses from exactly the same
samples of researchers over time. These surveys contained additional
questions matching some of those in the Zhang et al survey.</p>
</blockquote>
<ul>
<li><p><em>Update 2023/04/09:</em> (As brought up to me by a friend)
Most of the researchers surveyed were from academia, and only a few of
the researchers surveyed were from the currently-leading AI companies.
Since those companies seem to have a clear lead over academic AI
efforts, this leads to a bias where an important class of researchers
who are most familiar with the most advanced existing AI technology is
less represented. It's hard to say exactly which way this bias goes. It
could imply a bias towards a longer timeline since the advanced industry
researchers know how much further the technology already is secretly
ahead of what the public knows. Or, it could imply a bias towards a
shorter timeline since the advanced industry researchers are more
familiar with the limitations of current approaches while academics with
fewer details might expect that the recent rates of rapid advancement
should be expected to continue. Overall, I'll summarize this as a net
negligible bias that increases uncertainty.</p></li>
<li><p><em>Update 2023/04/30:</em> From OpenAI's CEO Sam Altman: "I
think we're at the end of the era where it's going to be these, like,
giant, giant models,” he told an audience at an event held at MIT late
last week. "We'll make them better in other ways." [<span><span
class="sidenote preview">wired<br/><em><a
href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/"
title="_blank"><img src="/favicon/wired.com.ico" class="favicon"
alt="/favicon/wired.com.ico" />https://www.wired.com/story/metas-vr-headset-quest-pro-personal-data-face/</a></em><br/>Placeholder
description for
https://www.wired.com/story/metas-vr-headset-quest-pro-personal-data-face/</span><a
href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/"><img
src="/favicon/wired.com.ico" class="favicon"
alt="/favicon/wired.com.ico" />wired</a></span>]. The immediate
implication is that recent major advances have been significantly driven
by scaling existing techniques, and so since that scaling's
effectiveness is imminently dropping off, fundamentally new techniques
are required in order to make progress. This development is evidence
that, at the time of the survey, those in the industry, who would be
more likely to predict this situation since they are more knowledgeable
about the cutting edge, would be influenced by this to predict longer
timelines to HLMI than the surveyed academics who would only be
influenced by extrapolating current trends about industry AI development
from an external perspective.</p></li>
</ul>
<p>Unfortunately, the survey didn't ask for a predicted timeline within
which respondents expect the effect of advanced AI on humanity to be
"extremely bad (e.g. human extinction)".</p>
<p>There are many biases to account for, as with any survey, but it
seems there should be something like a general trend of: - significant
underestimation of the time-to-HLMI - slight overestimation of the risk
of (eventual) AI Doomsday - higher uncertainty due to the most advanced
researchers in industry not being included as much as academics</p>
<p>Additionally, the variance among survey responses is very high. There
is nothing like a clear consensus on the near term (50 years), as this
<span><span class="sidenote preview">post by Our World In
Data<br/><em><a href="https://ourworldindata.org/ai-timelines"
title="_blank"><img src="/favicon/ourworldindata.org.ico"
class="favicon"
alt="/favicon/ourworldindata.org.ico" />https://ourworldindata.org/ai-timelines</a></em><br/>Placeholder
description for https://ourworldindata.org/ai-timelines</span><a
href="https://ourworldindata.org/ai-timelines"><img
src="/favicon/ourworldindata.org.ico" class="favicon"
alt="/favicon/ourworldindata.org.ico" />post by Our World In
Data</a></span> mentions. However, there is a relatively strong
consensus (90% of respondents) that HLMI will be developed within the
next 100 years. However, 100 years is a very long timeline for any
technological prediction, so I take it as having much less certainty
than a much more near-term prediction like 37 years.</p>
<p>With that said, consider how these results compare to the popular
imminent AI Doomsday predictions.</p>
<h3 id="timeline-to-hlmi">§ <a href="#timeline-to-hlmi"
class="no-link-favicon no-link-preview">Timeline to HLMI</a></h3>
<p>Imminent AI Doomsday predictions often rate the time-to-HLMI as even
shorter than this survey suggests and the likelihood of eventual
Doomsday as even higher. For example, Eliezer Yudkowsky -- one of the
main public imminent AI Doomsday predictors -- suggests in his
<span><span class="sidenote preview">TIME letter<br/><em><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"
title="_blank"><img src="/favicon/time.com.png" class="favicon"
alt="/favicon/time.com.png" />https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/</a></em><br/>Placeholder
description for
https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/</span><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"><img
src="/favicon/time.com.png" class="favicon"
alt="/favicon/time.com.png" />TIME letter</a></span> that imminent
Doomsday is extremely likely unless drastic and extremely unlikely
regulations on AI development are imposed soon i.e. we "shut it all
down". For example, he has recommended in various discussions (e.g. on
<span><span class="sidenote preview">Lex Friedman's podcast<br/><em><a
href="https://overcast.fm/+eZyDo-AY0" title="_blank"><img
src="/favicon/overcast.fm.ico" class="favicon"
alt="/favicon/overcast.fm.ico" />https://overcast.fm/+eZyDo-AY0</a></em><br/>Placeholder
description for https://overcast.fm/+eZyDo-AY0</span><a
href="https://overcast.fm/+eZyDo-AY0"><img
src="/favicon/overcast.fm.ico" class="favicon"
alt="/favicon/overcast.fm.ico" />Lex Friedman's podcast</a></span>) that
young people today should not put much weight on the future because of
how likely imminent AI Doomsday is (perhaps this was rhetorical and he
doesn't truly believe that the timeline is that sure, but my impression
is that he has never intentionally been rhetorical in any kind of way
similar to this so I doubt that he is in this instance either).</p>
<p>However, Yudkowsky has repeatedly refused to give a specific
probability distribution over the likelihood of imminent AI Doomsday
timelines. He explained this strategy on <span><span
class="sidenote preview">The Lunar Society podcast<br/><em><a
href="https://overcast.fm/+b53M_HRgo" title="_blank"><img
src="/favicon/overcast.fm.ico" class="favicon"
alt="/favicon/overcast.fm.ico" />https://overcast.fm/+eZyDo-AY0</a></em><br/>Placeholder
description for https://overcast.fm/+eZyDo-AY0</span><a
href="https://overcast.fm/+b53M_HRgo"><img
src="/favicon/overcast.fm.ico" class="favicon"
alt="/favicon/overcast.fm.ico" />The Lunar Society podcast</a></span>.
My summary of it is that he believes that publicly revealing specific
predictions would be misleading because: it's very difficult to predict
the particular pathway to imminent AI Doomsday, so his specific
predictions would likely be wrong (even if he correctly thought they
were more likely than people who do not predict imminent AI Doomsday),
so people would incorrectly infer that his prediction of imminent AI
Doomsday is unlikely even though the ultimate convergence of many
individually unlikely pathways is together very likely. Additionally, he
doesn't think there are any intermediate milestone predictions that he
expects to be convergent in the same way that people could use to
accurately judge his model that predicts AI Doomsday.</p>
<p>This is a coherent position to have. As he gives as an example,
predicting the lottery is also like this: it's very difficult to predict
<em>which</em> lottery ticket is the winning lottery ticket, but it's
very easy to predict that <em>some</em> winning ticket will be
drawn.</p>
<p>This position has as a premise that, before his ultimate prediction
of imminent AI Doomsday plays out (or perhaps until <em>right</em>
before imminent AI Doomsday), there are no intermediate predictions that
he could make that we could judge his model by. This implies that when
we observe certain events related to AI development that he doesn't
think immediately portent AI Doomsday, they should not impact his
predictions. Since if they did, then he could have made them as
intermediate predictions beforehand. However, as judged from his
comments on Twitter and in podcasts, it does seem like he's updating
regularly by judging particular AI behaviors (for example, Bing
threatening users with blackmail) to support his dire predictions.</p>
<p>My guess is that he's leaving a lot of credibility "on the table"
here; it would be easy for him to make near-term predictions about ways
that AI technology will act that most people would not expect. If he did
so, many people (including me) would take his concerns about eventual,
and to some extent, AI Doomsday more seriously.</p>
<p>In particular, he could bundle together in whatever way he likes the
classes of possible observations that would increase his expectation of
imminent AI Doomsday and predict that at least one of these things will
happen in some specified timeline. Even if each bundle item is unlikely,
he could find some disjunctive bundle that he thinks is somewhat close
to 50% likely and that some prominent people that disagree with him on
imminent AI Doomsday would put at much less than 50% likelihood. I hope
he or another imminent AI Doomsday predictor does something like
this.</p>
<h3
id="most-ai-researchers-expect-that-superintelligent-ai-will-not-be-extremely-bad">§
<a
href="#most-ai-researchers-expect-that-superintelligent-ai-will-not-be-extremely-bad"
class="no-link-favicon no-link-preview">Most AI Researchers Expect that
Superintelligent AI will <em>not</em> be Extremely Bad</a></h3>
<p>In the survey, the aggregate forecast time to a 50% chance of HLMI is
37 years, the median correspondent things that there is an "about even
chance" that an intelligence explosion will happen (withing 30 years of
HLMI) and they believe that the probability that the long-run effect of
advanced AI on humanity will be "extremely bad (e.g. human extinction)"
with probability 5%. - Note that this is within 1% of Scott Alexander's
<span><span class="sidenote preview">lizardman's constant<br/><em><a
href="https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/"
title="_blank"><img src="/favicon/slatestarcodex.com.ico"
class="favicon"
alt="/favicon/slatestarcodex.com.ico" />https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/</a></em><br/>Placeholder
description for
https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/</span><a
href="https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/"><img
src="/favicon/slatestarcodex.com.ico" class="favicon"
alt="/favicon/slatestarcodex.com.ico" />lizardman's constant</a></span>,
which is the observation that for most surveys, for responses that
indicate very unorthodox and unjustified beliefs, at least 4% of
respondents will give those responses (regardless of honesty).</p>
<p>With some slight extrapolation, the median correspondent believes
something close to this: it's fairly likely (~25%) that superintelligent
AI will be developed within ~67 years (50% chance of HLMI in 37 years,
then allowing for up to 30 years for the 50% chance of intelligence
explosion to play out), yet they also believe that the long-run effect
of advanced AI on humanity is likely (95%) not extremely bad.</p>
<p>This doesn't correspond well to the usual imminent AI Doomday
position, which predicts that superintelligent AI is <em>extremely
likely</em> to cause AI Doomsday. If an imminent AI Doomsday predictor
wants to use this data to support their claim about the short timeline,
they should also take into account, or adequately explain why they are
not, the aggregate position of the respondents that they aren't nearly
as worried about AI Doomsday once superintelligence is developed.</p>
<h3 id="my-current-position-on-imminent-ai-doomsday">§ <a
href="#my-current-position-on-imminent-ai-doomsday"
class="no-link-favicon no-link-preview">My Current Position on Imminent
AI Doomsday</a></h3>
<p>Almost all the arguments I've heard from imminent AI Doomsday
predictors focus almost on supporting the <em>eventual</em> AI Doomsday
argument and neglect or hand-wave the implications for <em>imminent</em>
AI Doomsday. Of course, the support for eventual AI Doomsday is
<em>some</em> support for imminent AI Doomsday, but not much.</p>
<p>For example, Tyler Cowen wrote a <span><span
class="sidenote preview">spirited post<br/><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"
title="_blank"><img src="/favicon/marginalrevolution.com.png"
class="favicon"
alt="/favicon/marginalrevolution.com.png" />https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</a></em><br/>Placeholder
description for
https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />spirited post</a></span>
claiming among other things that the imminent AI Doomsday predictors are
much too confident. Scott Alexander <span><span
class="sidenote preview">responded to Cowen<br/><em><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"
title="_blank"><img src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</a></em><br/>Placeholder
description for
https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</span><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"><img
src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />responded to Cowen</a></span> with the
claim that Cowen's argument was weak because it suffered from the "safe
uncertainty fallacy" -- that is, if the specifics of a potential danger
are not clear that does <em>not</em> necessarily imply that the dangers
are less likely. Alexander analogizes the situation to discovering that
an alien spaceship is heading towards Earth -- we don't know anything
about the aliens, so any particular predictions about what will happen
when they get here are uncertain, and yet we definitely should be
worried about the potential dangers because aliens are likely enough to
be dangerous enough in <em>some</em> way to warrant worry. I think
Alexander missed that point of Cowen's article, which it seems that
<span><span class="sidenote preview">Cowen agrees with me
about<br/><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"
title="_blank"><img src="/favicon/marginalrevolution.com.png"
class="favicon"
alt="/favicon/marginalrevolution.com.png" />https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</a></em><br/>Placeholder
description for
https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />Cowen agrees with me
about</a></span>, because Alexander's argument starts with the premise
that we already <em>know</em> that a big potential danger is on the
horizon, such as the alien spaceship. But with AI, Cowen <em>explicitly
disagrees</em> that AI poses such an immediate danger. Alexander
probably does disagree with Cowen about this, but he doesn't present an
argument for this premise in his article and so it doesn't make sense to
argue against Cowen by first assuming he is wrong.</p>
<p>However, I'm sure there are some good arguments for imminent AI
Doomsday out there, so I'll look for them.</p>
<p>As I discussed above, I do in fact believe in eventual AI Doomsday.
But my base rate expectation for something like that happening within
the next 20 years is very low. Before the most recent impressive public
advances in AI technology, ChatGPT and other <span><span
class="sidenote preview">large language models (LLM)<br/><em><a
href="https://en.wikipedia.org/wiki/Large_language_model"
title="_blank"><img src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />https://en.wikipedia.org/wiki/Escrow</a></em><br/>Placeholder
description for https://en.wikipedia.org/wiki/Escrow</span><a
href="https://en.wikipedia.org/wiki/Large_language_model"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />large language models
(LLM)</a></span> in particular, my expectation for HLMI in the next 20
years was that it was negligible, something like 1%. Nothing I'd seen
had indicated that state-of-the-art AI technology was on track for
anything like that without many breakthroughs. And, the field has been
in a lot of flux in the last decade or two to say the least, which
indicates that solid, compounding progress had not kicked in yet --
which is something important that I'd expect to see before the
development run up to HLMI.</p>
<p>I was very impressed by ChatGPT, using it for myself and seeing other
people online share what they could do with it. What impressed me was
the breadth of interesting things that could be accomplished by such a
fundamentally theory-lacking approach. The model powering ChatGPT is a
<em>huge</em> version of basically the same kind of model that has been
used over the last 5 years or so (the generative pre trained transformer
(GPT) was <span><span class="sidenote preview">introduced by Google
Brain in 2017<br/><em><a href="https://arxiv.org/abs/1706.03762"
title="_blank"><img src="/favicon/arxiv.org.png" class="favicon"
alt="/favicon/arxiv.org.png" />https://arxiv.org/abs/1706.03762</a></em><br/>Placeholder
description for https://arxiv.org/abs/1706.03762</span><a
href="https://arxiv.org/abs/1706.03762"><img
src="/favicon/arxiv.org.png" class="favicon"
alt="/favicon/arxiv.org.png" />introduced by Google Brain in
2017</a></span>), plus some human-powered fine-tuning (i.e. <span><span
class="sidenote preview">reinforcement learning from human feedback
(RLHF)<br/><em><a href="https://huggingface.co/blog/rlhf"
title="_blank"><img src="/favicon/huggingface.co.ico" class="favicon"
alt="/favicon/huggingface.co.ico" />https://huggingface.co/blog/rlhf</a></em><br/>Placeholder
description for https://huggingface.co/blog/rlhf</span><a
href="https://huggingface.co/blog/rlhf"><img
src="/favicon/huggingface.co.ico" class="favicon"
alt="/favicon/huggingface.co.ico" />reinforcement learning from human
feedback (RLHF)</a></span>). I only found ChatGPT to be a minor step
towards HLMI, if nothing else it shows that there are still meaningful
gains to be had in scaling current technologies. It has become a sort of
meme, among imminent AI Doomsday predictors in particular, to make fun
of people who claim that ChatGPT is just a "stochastic parrot", usually
by presenting a ChatGPT transcript and claiming its obviously "truly
thinking" or something like that. Yet, it still displays many telltale
signs of whatever "stochastic parrot" probably is, in that if you give
it challenging prompts that are unlike things from its training set, it
reliably fails; an AI system that was effectively generalizing and not
just doing relatively low-order pattern matching. For example: - ChatGPT
can do a lot of simple math problems, but if you expand a math problem
it can normally do ok to one that has a lot of complications and isn't
just a straightforward textbook problem then it struggles -- it will
give you <em>some</em> answers but with serious flaws - ChatGPT can do
some standard logical derivations, but cannot reason about a
non-standard logical system. If I present a new logical system, even
including a variety of examples of derivations in the new system,
ChatGPT forgets about the rules I specified and uses whatever standard
logical rules happen to have a similar form instead.</p>
<p>The step from ChatGPT to GPT4 was more subtle in terms of superficial
user experience. But, personally, my impression was that it responded
appropriately to my exact intentions <em>much</em> more often and
precisely than ChatGPT, especially in terms of respecting specific
details of my requests and giving more detail-oriented rather than very
general unassuming responses. For example: - GPT4 can write a short
story that weaves together a large number of specified elements without
forgetting any of them or hyper focussing on only a few.</p>
<p>In terms of distances from HLMI, GPT4 is the same order of magnitude
distances from it as ChatGPT i.e. I don't consider it a much larger step
towards HLMI than ChatGPT was. But, it was still surprising to me that
such a large improvement could be made in such short order. So overall
given these developments within the last year, I respectfully increase
my expectation that HLMI will happen in 20 years to 2%.</p>
<p><em>Update 2023/04/30:</em> I've since learned that GPT4 was actually
not developed so quickly after GPT3.5 as I previously thought. OpenAI's
Sam Altman stated that "we have had the initial training of GPT-4 done
for quite awhile, but it’s taken us a long time and a lot of work to
feel ready to release it." [<span><span
class="sidenote preview">twitter<br/><em><a
href="https://twitter.com/sama/status/1635687859494715393?s=20"
title="_blank"><img src="/favicon/twitter.com.ico" class="favicon"
alt="/favicon/twitter.com.ico" />https://twitter.com/sama/status/1635687859494715393?s=20</a></em><br/>Placeholder
description for
https://twitter.com/sama/status/1635687859494715393?s=20</span><a
href="https://twitter.com/sama/status/1635687859494715393?s=20"><img
src="/favicon/twitter.com.ico" class="favicon"
alt="/favicon/twitter.com.ico" />twitter</a></span>]. I haven't been
able to find more specific information generally available online, but
I've heard from Altman on podcast interviews that GPT4's "initial
training" was done around the time of the release of ChatGPT
(2022/11/30). So, the improvement from GPT3.5 to GPT4 wasn't actually
done as quickly as I previously thought, but that GPT4-level
capabilities were mostly already accomplished by the time that GPT3.5
was available. This implies that its harder to judge how quickly
capabilities improved from GPT3.5-level to GPT4-level.</p>
<h3 id="what-i-would-consider-a-major-shortcoming-in-my-reasoning">§ <a
href="#what-i-would-consider-a-major-shortcoming-in-my-reasoning"
class="no-link-favicon no-link-preview">What I Would Consider a Major
Shortcoming in My Reasoning</a></h3>
<p>A common argument for imminent AI Doomsday that I haven't addressed
is the "fast takeoff" or "foom" scenario: a certain level of AI
advancement that is achievable in the near-term future could lead to an
extremely quick feedback loop of self-improving AI systems, which within
a very short period advances to AI to superintelligence.</p>
<p>This sort of scenario is difficult to handle because it has very
little warning -- we might not know much about how likely it is to
happen until it's right about to or already happening. If it's possible
to solidly predict a non-negligible probability of fast-takeoff, I think
that's one of the most important things that imminent Doomsday
predictors could do to advance their case.</p>
<p>Abstracting away from the fast takeoff question, I find the main
feature lacking among imminent Doomsday arguments is detailed to support
specific probabilities. In-principle arguments for the in-principle
possibility and so <em>eventual</em> likelihood of AI Doomsday don't say
much about <em>imminent</em> Doomsday. If I was presented with a
compelling case for why specific features of AI systems can be
demonstrated to very clearly be extrapolated to follow a widely agreed
upon path towards superintelligence in the near term, I would be utterly
convinced. The fact that AI Doomsday predictors don't seem to spend much
effort trying to present specific ways in which current AI systems could
be extrapolated in this way, and instead focus on very unquantifiable
feelings about how scary and/or impressive some of the behaviors of AI
systems are, makes me skeptical. Notably, the researchers spending so
much time, effort, and money developing these systems haven't noticed
such an extrapolated pathway either. Given all these factors, and my
prior expectation that HLMI is very difficult, I think it's very
unlikely to exist.</p>
<p>Aside from this, advances in AI technology could also change my
mind.</p>
<h3 id="what-i-would-consider-significant-steps-towards-hlmi">§ <a
href="#what-i-would-consider-significant-steps-towards-hlmi"
class="no-link-favicon no-link-preview">What I Would Consider
Significant Steps Towards HLMI</a></h3>
<p><em>An underlying theory.</em> I would consider it a major step
towards HLMI if the field of AI research establishes some
widely-respected new theories about the nature of intelligence, probably
including the intelligence of animals at the least. Until then, I expect
more lumpy developments like the explosive rise of AI in public
consciousness recently as surprisingly effective techniques are stumbled
into. But my prior is that HLMI is so difficult that it will probably
not be "stumbled into" in this way.</p>
<p><em>Generality.</em> I would consider it a major step towards HLMI if
I can specify an arbitrary logical system (<em>especially</em> one that
is very unlike a standard logical system) and the AI system can reason
about it with relatively perfect accuracy (&gt;95%). This would
demonstrate that the system has fully encoded logical reasoning, and is
ready to replace math grad students (haha). In all seriousness, this
would be a critical development because it implies that you can give a
specification (to some degree informal, of course) of your desires and
have some high-level but still mathematically-formalizable expectations
about what the AI will do with it. Effectively abstracting the behavior
of components of systems is the foundation of scaling. Once you can
reliably break up tasks into pieces that the AI can perform up to some
spec, then you can compose them together and abstract hierarchies of
specifications that will allow for the design of organizations of AI
systems that mimic the kind of powerful abstract organization in
programming and society. Whole "governments" of AI systems could operate
autonomously with predictable behavior up to some requirements. This is
already <span><span class="sidenote preview">possible to some
extent<br/><em><a href="https://arxiv.org/abs/2303.17580"
title="_blank"><img src="/favicon/arxiv.org.png" class="favicon"
alt="/favicon/arxiv.org.png" />https://arxiv.org/abs/1706.03762</a></em><br/>Placeholder
description for https://arxiv.org/abs/1706.03762</span><a
href="https://arxiv.org/abs/2303.17580"><img
src="/favicon/arxiv.org.png" class="favicon"
alt="/favicon/arxiv.org.png" />possible to some extent</a></span>, but
with significant inherent limitations on abstraction and delegation.</p>
<h2 id="how-hard-is-it-to-avoid-ai-doomsday-aka-the-alignment-problem">§
<a href="#how-hard-is-it-to-avoid-ai-doomsday-aka-the-alignment-problem"
class="no-link-favicon no-link-preview">How Hard Is It To Avoid AI
Doomsday? (aka The Alignment Problem)</a></h2>
<p>Seems very hard.</p>
<p>I haven't given this question <em>as</em> much thought; I believe
that imminent AI Doomsday is unlikely, so I think we still have plenty
of time to consider it. Additionally, I expect that the details of AI
technology will change significantly in ways relevant to this question
before AI Doomsday <em>does</em> become imminent, so there's probably
not much progress we can make on answering this question until we have
much more advanced and understood AI technology.</p>
<h2 id="more-resource">§ <a href="#more-resource"
class="no-link-favicon no-link-preview">More Resource</a></h2>
<p>Below are some media on this topic that I recommend. One of the main
reasons why I wanted to write this post is because much of the
discussion of this topic I've seen seems very confused (or -- confusing
to me perhaps), and so I want to put down my own thoughts so that I can
get other people's pointed feedback and clearly track how my opinions
change.</p>
<ul>
<li><span><span class="sidenote preview">AiImpacts.org -- 2022 Expert
Survey on Progress in AI<br/><em><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
title="_blank"><img src="/favicon/aiimpacts.org.ico" class="favicon"
alt="/favicon/aiimpacts.org.ico" />https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</a></em><br/>Placeholder
description for
https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</span><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"><img
src="/favicon/aiimpacts.org.ico" class="favicon"
alt="/favicon/aiimpacts.org.ico" />AiImpacts.org -- 2022 Expert Survey
on Progress in AI</a></span></li>
<li><span><span class="sidenote preview">Robin Hanson -- current views
on AI danger<br/><em><a
href="https://open.substack.com/pub/overcomingbias/p/ai-risk-again"
title="_blank"><img src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</a></em><br/>Placeholder
description for
https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</span><a
href="https://open.substack.com/pub/overcomingbias/p/ai-risk-again"><img
src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />Robin Hanson -- current views on AI
danger</a></span></li>
<li><span><span class="sidenote preview">Erik Hoel -- how to navigate
the AI apocalypse<br/><em><a
href="https://open.substack.com/pub/erikhoel/p/how-to-navigate-the-ai-apocalypse"
title="_blank"><img src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</a></em><br/>Placeholder
description for
https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</span><a
href="https://open.substack.com/pub/erikhoel/p/how-to-navigate-the-ai-apocalypse"><img
src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />Erik Hoel -- how to navigate the AI
apocalypse</a></span></li>
<li><span><span class="sidenote preview">Tyler Cowen -- Existential
risk, AI, and the inevitable turn in human history<br/><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"
title="_blank"><img src="/favicon/marginalrevolution.com.png"
class="favicon"
alt="/favicon/marginalrevolution.com.png" />https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</a></em><br/>Placeholder
description for
https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />Tyler Cowen -- Existential
risk, AI, and the inevitable turn in human history</a></span></li>
<li><span><span class="sidenote preview">Tyler Cowen -- response to FLI
letter<br/><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html"
title="_blank"><img src="/favicon/marginalrevolution.com.png"
class="favicon"
alt="/favicon/marginalrevolution.com.png" />https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</a></em><br/>Placeholder
description for
https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />Tyler Cowen -- response to
FLI letter</a></span></li>
<li><span><span class="sidenote preview">Tyler Cowen -- against pausing
AI development<br/><em><a
href="https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves"
title="_blank"><img src="/missing.ico" class="favicon"
alt="/missing.ico" />https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves</a></em><br/>Placeholder
description for
https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves</span><a
href="https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves"><img
src="/missing.ico" class="favicon" alt="/missing.ico" />Tyler Cowen --
against pausing AI development</a></span></li>
<li><span><span class="sidenote preview">Tyler Cowen -- response to
ACX's response<br/><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"
title="_blank"><img src="/favicon/marginalrevolution.com.png"
class="favicon"
alt="/favicon/marginalrevolution.com.png" />https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</a></em><br/>Placeholder
description for
https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html</span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />Tyler Cowen -- response to
ACX's response</a></span></li>
<li><span><span class="sidenote preview">Eliezer Yudkowsky -- (response
to FLI letter) Pausing AI Developments Isn't Enough. We Need to Shut it
All Down<br/><em><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"
title="_blank"><img src="/favicon/time.com.png" class="favicon"
alt="/favicon/time.com.png" />https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/</a></em><br/>Placeholder
description for
https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/</span><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"><img
src="/favicon/time.com.png" class="favicon"
alt="/favicon/time.com.png" />Eliezer Yudkowsky -- (response to FLI
letter) Pausing AI Developments Isn't Enough. We Need to Shut it All
Down</a></span></li>
<li><span><span class="sidenote preview">Eliezer Yudkowsky -- Lex
Friedman podcast<br/><em><a href="https://overcast.fm/+eZyDo-AY0"
title="_blank"><img src="/favicon/overcast.fm.ico" class="favicon"
alt="/favicon/overcast.fm.ico" />https://overcast.fm/+eZyDo-AY0</a></em><br/>Placeholder
description for https://overcast.fm/+eZyDo-AY0</span><a
href="https://overcast.fm/+eZyDo-AY0"><img
src="/favicon/overcast.fm.ico" class="favicon"
alt="/favicon/overcast.fm.ico" />Eliezer Yudkowsky -- Lex Friedman
podcast</a></span></li>
<li><span><span class="sidenote preview">Eliezer Yudkowsky -- Lunar
Society podcast<br/><em><a href="https://overcast.fm/+b53M_HRgo"
title="_blank"><img src="/favicon/overcast.fm.ico" class="favicon"
alt="/favicon/overcast.fm.ico" />https://overcast.fm/+eZyDo-AY0</a></em><br/>Placeholder
description for https://overcast.fm/+eZyDo-AY0</span><a
href="https://overcast.fm/+b53M_HRgo"><img
src="/favicon/overcast.fm.ico" class="favicon"
alt="/favicon/overcast.fm.ico" />Eliezer Yudkowsky -- Lunar Society
podcast</a></span></li>
<li><span><span class="sidenote preview">ACX -- predictions of AI
apocalypse<br/><em><a
href="https://open.substack.com/pub/astralcodexten/p/why-i-am-not-as-much-of-a-doomer"
title="_blank"><img src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</a></em><br/>Placeholder
description for
https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</span><a
href="https://open.substack.com/pub/astralcodexten/p/why-i-am-not-as-much-of-a-doomer"><img
src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />ACX -- predictions of AI
apocalypse</a></span></li>
<li><span><span class="sidenote preview">ACX -- response to Tyler
Cowen<br/><em><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"
title="_blank"><img src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</a></em><br/>Placeholder
description for
https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</span><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"><img
src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />ACX -- response to Tyler
Cowen</a></span></li>
<li><span><span class="sidenote preview">Zvi -- response to Tyler
Cowen<br/><em><a
href="https://open.substack.com/pub/thezvi/p/response-to-tyler-cowens-existential"
title="_blank"><img src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</a></em><br/>Placeholder
description for
https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</span><a
href="https://open.substack.com/pub/thezvi/p/response-to-tyler-cowens-existential"><img
src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />Zvi -- response to Tyler
Cowen</a></span></li>
<li><span><span class="sidenote preview">Zvi -- reponse to FLI
letter<br/><em><a
href="https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter"
title="_blank"><img src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter</a></em><br/>Placeholder
description for
https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter</span><a
href="https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter"><img
src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />Zvi -- reponse to FLI
letter</a></span></li>
<li><span><span class="sidenote preview">Zvi -- response to Eliezer
Yudkowsky's TIME article<br/><em><a
href="https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time"
title="_blank"><img src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter</a></em><br/>Placeholder
description for
https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter</span><a
href="https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time"><img
src="/favicon/substack.com.svg" class="favicon"
alt="/favicon/substack.com.svg" />Zvi -- response to Eliezer Yudkowsky's
TIME article</a></span></li>
</ul>
<h2 id="references">§ <a href="#references"
class="no-link-favicon no-link-preview">References</a></h2>
<ul>
<li><a href="/asset/ai-danger/ai-danger-by-dalle-2.png"
class="inReference no-link-preview"><img src="/favicon.ico"
class="favicon" alt="/favicon.ico" />illustration of "AI danger" by
Dalle 2</a></li>
<li><a href="https://lifearchitect.ai/timeline"
class="inReference no-link-preview"><img
src="/favicon/lifearchitect.ai.ico" class="favicon"
alt="/favicon/lifearchitect.ai.ico" />recent advancements</a></li>
<li><a href="http://openai.com/"
class="inReference no-link-preview"><img src="/favicon/openai.com.svg"
class="favicon" alt="/favicon/openai.com.svg" />Open AI</a></li>
<li><a
href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"
class="inReference no-link-preview"><img
src="/favicon/futureoflife.org.png" class="favicon"
alt="/favicon/futureoflife.org.png" />Pause Giant AI Experiments: An
Open Letter</a></li>
<li><a
href="https://futureoflife.org/ai/faqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments/"
class="inReference no-link-preview"><img
src="/favicon/futureoflife.org.png" class="favicon"
alt="/favicon/futureoflife.org.png" />FAQ</a></li>
<li><a
href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/"
class="inReference no-link-preview"><img src="/favicon/reuters.com.ico"
class="favicon" alt="/favicon/reuters.com.ico" />reuters</a></li>
<li><a href="https://en.wikipedia.org/wiki/Nick_Bostrom"
class="inReference no-link-preview"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />Nick Bostrom</a></li>
<li><a
href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"
class="inReference no-link-preview"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />Superintelligence</a></li>
<li><a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky"
class="inReference no-link-preview"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />Eliezer Yudkowsky</a></li>
<li><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
class="inReference no-link-preview"><img
src="/favicon/aiimpacts.org.ico" class="favicon"
alt="/favicon/aiimpacts.org.ico" />2022 Expert Survey on Progress in
AI</a></li>
<li><a
href="https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/"
class="inReference no-link-preview"><img
src="/favicon/aiimpacts.org.ico" class="favicon"
alt="/favicon/aiimpacts.org.ico" />2016 version of this survey</a></li>
<li><a
href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/"
class="inReference no-link-preview"><img src="/favicon/wired.com.ico"
class="favicon" alt="/favicon/wired.com.ico" />wired</a></li>
<li><a href="https://ourworldindata.org/ai-timelines"
class="inReference no-link-preview"><img
src="/favicon/ourworldindata.org.ico" class="favicon"
alt="/favicon/ourworldindata.org.ico" />post by Our World In
Data</a></li>
<li><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"
class="inReference no-link-preview"><img src="/favicon/time.com.png"
class="favicon" alt="/favicon/time.com.png" />TIME letter</a></li>
<li><a href="https://overcast.fm/+eZyDo-AY0"
class="inReference no-link-preview"><img src="/favicon/overcast.fm.ico"
class="favicon" alt="/favicon/overcast.fm.ico" />Lex Friedman's
podcast</a></li>
<li><a href="https://overcast.fm/+b53M_HRgo"
class="inReference no-link-preview"><img src="/favicon/overcast.fm.ico"
class="favicon" alt="/favicon/overcast.fm.ico" />The Lunar Society
podcast</a></li>
<li><a href="https://overcast.fm/+b53M_HRgo"
class="inReference no-link-preview"><img src="/favicon/overcast.fm.ico"
class="favicon" alt="/favicon/overcast.fm.ico" />The Lunar Society
podcast</a></li>
<li><a href="https://overcast.fm/+eZyDo-AY0"
class="inReference no-link-preview"><img src="/favicon/overcast.fm.ico"
class="favicon" alt="/favicon/overcast.fm.ico" />Lex Friedman
podcast</a></li>
<li><a href="https://gwern.net" class="inReference no-link-preview"><img
src="/favicon/gwern.net.png" class="favicon"
alt="/favicon/gwern.net.png" />Gwern Branwen</a></li>
<li><a
href="https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/"
class="inReference no-link-preview"><img
src="/favicon/slatestarcodex.com.ico" class="favicon"
alt="/favicon/slatestarcodex.com.ico" />lizardman's constant</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"
class="inReference no-link-preview"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />spirited post</a></li>
<li><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"
class="inReference no-link-preview"><img src="/favicon/substack.com.svg"
class="favicon" alt="/favicon/substack.com.svg" />responded to
Cowen</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"
class="inReference no-link-preview"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />Cowen agrees with me
about</a></li>
<li><a href="https://en.wikipedia.org/wiki/Large_language_model"
class="inReference no-link-preview"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />large language models (LLM)</a></li>
<li><a href="https://arxiv.org/abs/1706.03762"
class="inReference no-link-preview"><img src="/favicon/arxiv.org.png"
class="favicon" alt="/favicon/arxiv.org.png" />introduced by Google
Brain in 2017</a></li>
<li><a href="https://huggingface.co/blog/rlhf"
class="inReference no-link-preview"><img
src="/favicon/huggingface.co.ico" class="favicon"
alt="/favicon/huggingface.co.ico" />reinforcement learning from human
feedback (RLHF)</a></li>
<li><a href="https://twitter.com/sama/status/1635687859494715393?s=20"
class="inReference no-link-preview"><img src="/favicon/twitter.com.ico"
class="favicon" alt="/favicon/twitter.com.ico" />twitter</a></li>
<li><a href="https://arxiv.org/abs/2303.17580"
class="inReference no-link-preview"><img src="/favicon/arxiv.org.png"
class="favicon" alt="/favicon/arxiv.org.png" />possible to some
extent</a></li>
<li><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
class="inReference no-link-preview"><img
src="/favicon/aiimpacts.org.ico" class="favicon"
alt="/favicon/aiimpacts.org.ico" />AiImpacts.org -- 2022 Expert Survey
on Progress in AI</a></li>
<li><a
href="https://open.substack.com/pub/overcomingbias/p/ai-risk-again"
class="inReference no-link-preview"><img src="/favicon/substack.com.svg"
class="favicon" alt="/favicon/substack.com.svg" />Robin Hanson --
current views on AI danger</a></li>
<li><a
href="https://open.substack.com/pub/erikhoel/p/how-to-navigate-the-ai-apocalypse"
class="inReference no-link-preview"><img src="/favicon/substack.com.svg"
class="favicon" alt="/favicon/substack.com.svg" />Erik Hoel -- how to
navigate the AI apocalypse</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"
class="inReference no-link-preview"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />Tyler Cowen -- Existential
risk, AI, and the inevitable turn in human history</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html"
class="inReference no-link-preview"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />Tyler Cowen -- response to
FLI letter</a></li>
<li><a
href="https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves"
class="inReference no-link-preview"><img src="/missing.ico"
class="favicon" alt="/missing.ico" />Tyler Cowen -- against pausing AI
development</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"
class="inReference no-link-preview"><img
src="/favicon/marginalrevolution.com.png" class="favicon"
alt="/favicon/marginalrevolution.com.png" />Tyler Cowen -- response to
ACX's response</a></li>
<li><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"
class="inReference no-link-preview"><img src="/favicon/time.com.png"
class="favicon" alt="/favicon/time.com.png" />Eliezer Yudkowsky --
(response to FLI letter) Pausing AI Developments Isn't Enough. We Need
to Shut it All Down</a></li>
<li><a href="https://overcast.fm/+eZyDo-AY0"
class="inReference no-link-preview"><img src="/favicon/overcast.fm.ico"
class="favicon" alt="/favicon/overcast.fm.ico" />Eliezer Yudkowsky --
Lex Friedman podcast</a></li>
<li><a href="https://overcast.fm/+b53M_HRgo"
class="inReference no-link-preview"><img src="/favicon/overcast.fm.ico"
class="favicon" alt="/favicon/overcast.fm.ico" />Eliezer Yudkowsky --
Lunar Society podcast</a></li>
<li><a
href="https://open.substack.com/pub/astralcodexten/p/why-i-am-not-as-much-of-a-doomer"
class="inReference no-link-preview"><img src="/favicon/substack.com.svg"
class="favicon" alt="/favicon/substack.com.svg" />ACX -- predictions of
AI apocalypse</a></li>
<li><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"
class="inReference no-link-preview"><img src="/favicon/substack.com.svg"
class="favicon" alt="/favicon/substack.com.svg" />ACX -- response to
Tyler Cowen</a></li>
<li><a
href="https://open.substack.com/pub/thezvi/p/response-to-tyler-cowens-existential"
class="inReference no-link-preview"><img src="/favicon/substack.com.svg"
class="favicon" alt="/favicon/substack.com.svg" />Zvi -- response to
Tyler Cowen</a></li>
<li><a
href="https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter"
class="inReference no-link-preview"><img src="/favicon/substack.com.svg"
class="favicon" alt="/favicon/substack.com.svg" />Zvi -- reponse to FLI
letter</a></li>
<li><a
href="https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time"
class="inReference no-link-preview"><img src="/favicon/substack.com.svg"
class="favicon" alt="/favicon/substack.com.svg" />Zvi -- response to
Eliezer Yudkowsky's TIME article</a></li>
</ul>
<h2 id="signature">§ <a href="#signature"
class="no-link-favicon no-link-preview">Signature</a></h2>
<p>The following code block is the <a
href="https://en.wikipedia.org/wiki/EdDSA#Ed25519" title="_blank"><img
src="/favicon/wikipedia.org.ico" class="favicon"
alt="/favicon/wikipedia.org.ico" />Ed25519 signature</a> of this post's
<a href="/post_markdown/ai-danger.md"><img src="/favicon.ico"
class="favicon" alt="/favicon.ico" />markdown content</a> encoded in
base 64, using my <em>secret key</em> and <a
href="/key/main_ed25519.pub.txt" title="_blank"><img src="/favicon.ico"
class="favicon" alt="/favicon.ico" />public key</a>.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>c2b2936ff133e1887b27cc12c2aa97d5dde05065ce15d4b4cf477c4d5f4c69ae66d62d36c3b118e26425cd189ff471760ea2d155aeea1789d9479d0699e95409</span></code></pre></div>
<p>See <a href="/page/signature.html"><img src="/favicon.ico"
class="favicon" alt="/favicon.ico" />Signature</a> for more
information.</p></main>

    <footer>
      <div class="title">
        <div class="root-title">
          <a href="/">rybl.net</a>
        </div>

        <img class="website-icon" src="/image/rybl-small.png" />

        <div class="local-title">AI Danger</div>
      </div>
      <div class="menu">
        <div class="item">
          <a href="/">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
              viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
              stroke-linecap="round" stroke-linejoin="round"
              class="lucide lucide-circle-slash-icon lucide-circle-slash">
              <circle cx="12" cy="12" r="10" />
              <line x1="9" x2="15" y1="15" y2="9" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/library.html">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
              viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
              stroke-linecap="round" stroke-linejoin="round"
              class="icon lucide lucide-library-icon lucide-library">
              <path d="m16 6 4 14" />
              <path d="M12 6v14" />
              <path d="M8 8v12" />
              <path d="M4 4v16" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/about.html">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
              viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
              stroke-linecap="round" stroke-linejoin="round"
              class="icon lucide lucide-info-icon lucide-info">
              <circle cx="12" cy="12" r="10" />
              <path d="M12 16v-4" />
              <path d="M12 8h.01" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/profile.html">
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20"
              viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
              stroke-linecap="round" stroke-linejoin="round"
              class="lucide lucide-globe-icon lucide-globe">
              <circle cx="12" cy="12" r="10" />
              <path d="M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20" />
              <path d="M2 12h20" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/references-graph.html">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
              viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
              stroke-linecap="round" stroke-linejoin="round"
              class="lucide lucide-orbit-icon lucide-orbit">
              <path d="M20.341 6.484A10 10 0 0 1 10.266 21.85" />
              <path d="M3.659 17.516A10 10 0 0 1 13.74 2.152" />
              <circle cx="12" cy="12" r="3" />
              <circle cx="19" cy="5" r="2" />
              <circle cx="5" cy="19" r="2" />
            </svg>
          </a>
        </div>
        <div class="item">
          <a href="/page/signature.html"><svg xmlns="http://www.w3.org/2000/svg"
              width="24" height="24" viewBox="0 0 24 24" fill="none"
              stroke="currentColor" stroke-width="1" stroke-linecap="round"
              stroke-linejoin="round"
              class="lucide lucide-fingerprint-icon lucide-fingerprint">
              <path d="M12 10a2 2 0 0 0-2 2c0 1.02-.1 2.51-.26 4" />
              <path d="M14 13.12c0 2.38 0 6.38-1 8.88" />
              <path d="M17.29 21.02c.12-.6.43-2.3.5-3.02" />
              <path d="M2 12a10 10 0 0 1 18-6" />
              <path d="M2 16h.01" />
              <path d="M21.8 16c.2-2 .131-5.354 0-6" />
              <path d="M5 19.5C5.5 18 6 15 6 12a6 6 0 0 1 .34-2" />
              <path d="M8.65 22c.21-.66.45-1.32.57-2" />
              <path d="M9 6.8a6 6 0 0 1 9 5.2v2" />
            </svg></a>
        </div>
      </div></footer>  </body>
</html>
