<!doctype html>
<html>
  <head>
    <title>AI Danger</title>

    <link rel="stylesheet" href="/style/common.css" />
    <link rel="stylesheet" href="/style/header.css" />
    <link rel="stylesheet" href="/style/footer.css" />
    <link rel="stylesheet" href="/style/post.css" />

    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"
    ></script>
  </head>
  <body>
    <svg style="display: none; visibility: hidden">
      <defs>
        <filter
          id="body-background-filter"
          color-interpolation-filters="linearRGB"
          filterUnits="objectBoundingBox"
          primitiveUnits="userSpaceOnUse"
        >
          <feTurbulence
            type="fractalNoise"
            baseFrequency="0.035 0.008"
            numOctaves="4"
            seed="2"
            stitchTiles="stitch"
            result="turbulence"
          />
          <feMerge result="merge">
            <feMergeNode in="SourceGraphic" result="mergeNode" />
            <feMergeNode in="turbulence" result="mergeNode1" />
          </feMerge>
          <feColorMatrix
            type="saturate"
            values="10"
            in="merge"
            result="colormatrix"
          />
          <feDisplacementMap
            in="merge"
            in2="colormatrix"
            scale="1"
            xChannelSelector="R"
            yChannelSelector="R"
            result="displacementMap"
          />
          <feComposite
            in="displacementMap"
            in2="SourceAlpha"
            operator="in"
            result="composite1"
          />
        </filter>
      </defs>
    </svg>

    <div class="body-background"></div>    <header>This is the header as a template!</header>    <main>
      <div class="main-inner"><h2><a href="/post/ai-danger.html" data-noLinkFavicon=""
data-noLinkPreview="">AI Danger</a></h2>
<div class="header-info">
<p><u>Published:</u> 2023-04-03</p>
<p><u>Tags:</u> ai</p>
<p><u>Abstract.</u></p>
<p>My thoughts on the dangers of AI technology.</p>
</div>
<div class="table-of-contents">
<ol type="1">
<li><a href="#what-are-the-dangers-of-ai" data-noLinkFavicon=""
data-noLinkPreview="">What are the Dangers of AI?</a></li>
<li><a href="#how-likely-is-ai-doomsday" data-noLinkFavicon=""
data-noLinkPreview="">How Likely Is AI Doomsday?</a>
<ol type="1">
<li><a href="#important-reservations" data-noLinkFavicon=""
data-noLinkPreview="">Important Reservations</a></li>
</ol></li>
<li><a href="#how-likely-is-ai-doomsday-soon" data-noLinkFavicon=""
data-noLinkPreview="">How Likely Is AI Doomsday, Soon?</a>
<ol type="1">
<li><a href="#timeline-to-hlmi" data-noLinkFavicon=""
data-noLinkPreview="">Timeline to HLMI</a></li>
<li><a
href="#most-ai-researchers-expect-that-superintelligent-ai-will-not-be-extremely-bad"
data-noLinkFavicon="" data-noLinkPreview="">Most AI Researchers Expect
that Superintelligent AI will <em>not</em> be Extremely Bad</a></li>
<li><a href="#my-current-position-on-imminent-ai-doomsday"
data-noLinkFavicon="" data-noLinkPreview="">My Current Position on
Imminent AI Doomsday</a></li>
<li><a href="#what-i-would-consider-a-major-shortcoming-in-my-reasoning"
data-noLinkFavicon="" data-noLinkPreview="">What I Would Consider a
Major Shortcoming in My Reasoning</a></li>
<li><a href="#what-i-would-consider-significant-steps-towards-hlmi"
data-noLinkFavicon="" data-noLinkPreview="">What I Would Consider
Significant Steps Towards HLMI</a></li>
</ol></li>
<li><a
href="#how-hard-is-it-to-avoid-ai-doomsday-aka-the-alignment-problem"
data-noLinkFavicon="" data-noLinkPreview="">How Hard Is It To Avoid AI
Doomsday? (aka The Alignment Problem)</a></li>
<li><a href="#more-resource" data-noLinkFavicon=""
data-noLinkPreview="">More Resource</a></li>
<li><a href="#references" data-noLinkFavicon=""
data-noLinkPreview="">References</a></li>
<li><a href="#citations" data-noLinkFavicon=""
data-noLinkPreview="">Citations</a></li>
</ol>
</div>
<p><em>Definition</em>. The <strong>AI Doomsday</strong> scenario is a
possible future scenario where an AI system is developed that overcomes
all attempts at control and very soon after the system causes the
extinction of the human species. This definition abstracts away any
particular prediction of <em>when</em> this scenario will happen and
<em>how</em> specifically it will play out.</p>
<p><img src="/image/ai-danger-by-dalle-2.png"
alt="illustration of &quot;AI danger&quot; by Dalle 2" /></p>
<h2 id="what-are-the-dangers-of-ai">What are the Dangers of AI?</h2>
<p>AI has been a hot topic recently due to some impressive <span><a
href="https://lifearchitect.ai/timeline"><img
src="/favicon/https%253A%252F%252Flifearchitect.ai%252Ftimeline.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Flifearchitect.ai%2Ftimeline.ico" />recent
advancements</a><span class="sidenote preview"><span
class="preview-title"><em><a href="https://lifearchitect.ai/timeline"
title="_blank"><img
src="/favicon/https%253A%252F%252Flifearchitect.ai%252Ftimeline.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Flifearchitect.ai%2Ftimeline.ico" />https://lifearchitect.ai/timeline</a></em></span><span
class="preview-description">Placeholder description for
https://lifearchitect.ai/timeline</span></span></span>. In particular,
the chatbot web app <em>ChatGPT</em> by <span><a
href="http://openai.com/"><img
src="/favicon/http%253A%252F%252Fopenai.com%252F.svg" class="favicon"
alt="/favicon/http%3A%2F%2Fopenai.com%2F.svg" />Open AI</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="http://openai.com/" title="_blank"><img
src="/favicon/http%253A%252F%252Fopenai.com%252F.svg" class="favicon"
alt="/favicon/http%3A%2F%2Fopenai.com%2F.svg" />http://openai.com/</a></em></span><span
class="preview-description">Placeholder description for
http://openai.com/</span></span></span> has given the public a first
view of a new kind of AI technology that can be integrated into one's
everyday life and demonstrates surprisingly human-like capabilities.</p>
<p>On the internet I've seen a large variety of responses to the new and
quickly improving AI technology.</p>
<p>Many people are interested in how to use the technology to build
newly-possible tools and products.</p>
<p>Many people are concerned with the dangers brought by this new
technology that society at large is ill-prepared for. Overall, some main
(not entirely independent) concerns I've noticed are: - AI will soon be
able to perform many jobs more efficiently (in terms of meeting a
threshold quality/cost for employers) than many humans, which will lead
to mass unemployment. - AI will enable more proliferation of
misinformation, which will lead to further breakdown of social/political
institutions/relations. - AI will lead to a massive increase in
AI-generated content (writing, digital art, etc) that is hard to
distinguish from humans, which will decrease the value of
human-generated content and promote bullshit. - An AI system will be
developed that overcomes all attempts at control and soon after the
system causes the extinction of the human species (this is the <em>AI
Doomsday</em> scenario)</p>
<p>Many people are in favor of focusing pre-emptively on addressing
these dangers before they fully play out. For example, the letter
<em><span><a
href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"><img
src="/favicon/https%253A%252F%252Ffutureoflife.org%252Fopen-letter%252Fpause-giant-ai-experiments%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ffutureoflife.org%2Fopen-letter%2Fpause-giant-ai-experiments%2F.png" />Pause
Giant AI Experiments: An Open Letter</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"
title="_blank"><img
src="/favicon/https%253A%252F%252Ffutureoflife.org%252Fopen-letter%252Fpause-giant-ai-experiments%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ffutureoflife.org%2Fopen-letter%2Fpause-giant-ai-experiments%2F.png" />https://futureoflife.org/open-letter/pause-giant-ai-experiments/</a></em></span><span
class="preview-description">Placeholder description for
https://futureoflife.org/open-letter/pause-giant-ai-experiments/</span></span></span></em>
(and associated <span><a
href="https://futureoflife.org/ai/faqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments/"><img
src="/favicon/https%253A%252F%252Ffutureoflife.org%252Fai%252Ffaqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ffutureoflife.org%2Fai%2Ffaqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments%2F.png" />FAQ</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://futureoflife.org/ai/faqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments/"
title="_blank"><img
src="/favicon/https%253A%252F%252Ffutureoflife.org%252Fai%252Ffaqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ffutureoflife.org%2Fai%2Ffaqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments%2F.png" />https://futureoflife.org/ai/faqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments/</a></em></span><span
class="preview-description">Placeholder description for
https://futureoflife.org/ai/faqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments/</span></span></span>)
from the Future of Life Institute calls for a voluntary 6-month "pause"
on AI development by the AI industry leaders (in particular, Open
AI/Microsoft and Google) to call attention to and give time for efforts
to address the near-term dangers of AI.</p>
<p>Pausing AI development for even just 6 months is a <em>huge</em> ask.
Companies like Open AI, Microsoft, Google, Anthropic, and Midjourney are
spending billions of USD to develop these technologies and produce
billions of USD of value. The most recent AI developments have already
gained widespread use e.g. Open AI's ChatGPT accumulated at least 100
million users in just 2 months [<span><a
href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/"><img
src="/favicon/https%253A%252F%252Fwww.reuters.com%252Ftechnology%252Fchatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.reuters.com%2Ftechnology%2Fchatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01%2F.ico" />reuters</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.reuters.com%252Ftechnology%252Fchatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.reuters.com%2Ftechnology%2Fchatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01%2F.ico" />https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/</a></em></span><span
class="preview-description">Placeholder description for
https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/</span></span></span>].</p>
<p>The letter justifies this huge ask by pointing out the huge dangers
of AI technology, in particular:</p>
<ul>
<li>mass unemployment</li>
<li>mass misinformation</li>
<li>"loss of control of our civilization" (likely AI Doomsday)</li>
</ul>
<p>In addition to the impact of these dangers when they play out, the
probability distribution of various scales of these dangers is also
critical for weighing this justification.</p>
<p>There are surely many other dangers to AI technology, but in this
post, I will focus primarily on AI Doomsday.</p>
<h2 id="how-likely-is-ai-doomsday">How Likely Is AI Doomsday?</h2>
<p><em>Definition</em>. <strong>Eventual AI Doomsday</strong> is the
prediction that <strong>AI Doomsday</strong> will happen at some point
in the future.</p>
<p>In principle, AI Doomsday is very likely to happen. <span><a
href="https://en.wikipedia.org/wiki/Nick_Bostrom"><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FNick_Bostrom.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FNick_Bostrom.ico" />Nick
Bostrom</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://en.wikipedia.org/wiki/Nick_Bostrom" title="_blank"><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FNick_Bostrom.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FNick_Bostrom.ico" />https://en.wikipedia.org/wiki/Nick_Bostrom</a></em></span><span
class="preview-description">Placeholder description for
https://en.wikipedia.org/wiki/Nick_Bostrom</span></span></span>
popularized this observation in <span><a
href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FSuperintelligence%253A_Paths%252C_Dangers%252C_Strategies.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSuperintelligence%3A_Paths%2C_Dangers%2C_Strategies.ico" />Superintelligence</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"
title="_blank"><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FSuperintelligence%253A_Paths%252C_Dangers%252C_Strategies.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSuperintelligence%3A_Paths%2C_Dangers%2C_Strategies.ico" />https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies</a></em></span><span
class="preview-description">Placeholder description for
https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies</span></span></span>,
and <span><a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky"><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FEliezer_Yudkowsky.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FEliezer_Yudkowsky.ico" />Eliezer
Yudkowsky</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky"
title="_blank"><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FEliezer_Yudkowsky.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FEliezer_Yudkowsky.ico" />https://en.wikipedia.org/wiki/Eliezer_Yudkowsky</a></em></span><span
class="preview-description">Placeholder description for
https://en.wikipedia.org/wiki/Eliezer_Yudkowsky</span></span></span> has
spent much of his time rebutting his own and others' suggestions for how
to prevent an advanced AI system that <em>could</em> cause AI Doomsday
from doing so. I find the most convincing argument for AI Doomsday to be
the most abstract version. The argument has two parts:</p>
<ol>
<li><em>A sufficiently advanced AI system can cause AI Doomsday</em>.
Suppose there exists at some point an AI system that is so capable that
if human efforts to maintain control over it fail then it will never be
controllable again by humans after that point. Then it is very likely
that there will be some point after this development that the AI does
escape control in this way, and pursues its own goals. It is vanishingly
unlikely that the system's pursuit of these goals will leave room for
human survival.</li>
<li><em>Such a sufficiently advanced AI is likely to be developed</em>.
Suppose that the advancement of AI technology continues to be
non-negligible. Then at some point, human-level AI systems will be
developed. These systems will be able to aid in the development of more
advanced AI systems. So, it is very likely that the development of AI
technology will continue after that. If this trend continues
non-negligibly, it is likely that eventually, the technology will
develop to a capability that satisfies part 1 of this argument.</li>
</ol>
<p>This argument relies on a few premises: - The advancement of AI
technology continues, however sporadically, until human-level AI is
developed - AI advancement, especially after achieving human-level AI,
will outpace efforts to keep the AI systems under sufficient human
control to prevent part 1 from playing out - Such a capable AI system
beyond human control as in part 1 would pursue goals that would not
include human survival</p>
<p>Of course, it's difficult to directly judge the likelihood of these
premises in the <em>foreseeable</em> future, but over the long term of
humanity's <em>entire</em> future, they seem very likely in principle.
For example, there is a rough upper bound on the difficulty of
developing human-level AI: the possibility of artificially replicating
human intelligence based exactly on biological human intelligence (i.e.
digitally simulating a brain with sufficient detail to yield
intelligence).</p>
<p>I <em>do</em> believe the eventual AI Doomsday prediction is more
likely than not. I am very unsure, so a specific number is not very
useful, but I'd put it at something like a normal distribution of around
80% with a standard deviation of 20%.</p>
<p>TODO: give argument for claim: "It is vanishingly unlikely that the
system's pursuit of these goals will leave room for human survival."</p>
<h3 id="important-reservations">Important Reservations</h3>
<p>This is only an argument for an <em>eventual</em> AI Doomsday. It
does not distinguish between an imminent AI Doomsday that happens in 20
years or a long-term AI Doomsday that happens in 20,000 years. And the
argument by itself does not necessarily imply any significant
predictions about the distribution over these timelines.</p>
<p>This argument only makes reference to <em>intelligence</em>, which is
the ability of a system to accomplish general classes of complex goals.
This may turn out to have some relation to <em>consciousness</em>, but
that is a separate topic and the argument doesn't depend on any details
there. And so, the argument does not require any premises about imbuing
an AI system with consciousness.</p>
<h2 id="how-likely-is-ai-doomsday-soon">How Likely Is AI Doomsday,
Soon?</h2>
<p><em>Definition</em>. <strong>Imminent AI Doomsday</strong> is the
prediction that <strong>AI Doomsday</strong> will happen within the next
couple of decades. Within that timeline, there is a variety of popular
probability distributions.</p>
<p>I have seen this survey -- <span><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"><img
src="/favicon/https%253A%252F%252Faiimpacts.org%252F2022-expert-survey-on-progress-in-ai%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Faiimpacts.org%2F2022-expert-survey-on-progress-in-ai%2F.ico" />2022
Expert Survey on Progress in AI</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
title="_blank"><img
src="/favicon/https%253A%252F%252Faiimpacts.org%252F2022-expert-survey-on-progress-in-ai%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Faiimpacts.org%2F2022-expert-survey-on-progress-in-ai%2F.ico" />https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</a></em></span><span
class="preview-description">Placeholder description for
https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</span></span></span>
-- often cited to support the claim that most AI researchers expect
artificial general intelligence (AGI) to be developed in the near
future. It's also interesting to compare these results to <span><a
href="https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/"><img
src="/favicon/https%253A%252F%252Faiimpacts.org%252F2016-expert-survey-on-progress-in-ai%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Faiimpacts.org%2F2016-expert-survey-on-progress-in-ai%2F.ico" />2016
version of this survey</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/"
title="_blank"><img
src="/favicon/https%253A%252F%252Faiimpacts.org%252F2016-expert-survey-on-progress-in-ai%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Faiimpacts.org%2F2016-expert-survey-on-progress-in-ai%2F.ico" />https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/</a></em></span><span
class="preview-description">Placeholder description for
https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/</span></span></span>.
Quoting a few main results of the 2022 survey:</p>
<ol>
<li><strong>The aggregate forecast time to a 50% chance of HLMI was 37
years</strong>, i.e. 2059 (not including data from questions about the
conceptually similar Full Automation of Labor, which in 2016 received
much later estimates). This timeline has become about eight years
shorter in the six years since 2016, when the aggregate prediction put a
50% probability at 2061, i.e. 45 years out. Note that these estimates
are conditional on "human scientific activity continu[ing] without major
negative disruption."</li>
<li><strong>The median respondent believes the probability that the
long-run effect of advanced AI on humanity will be "extremely bad (e.g.,
human extinction)" is 5%.</strong> This is the same as it was in 2016
(though Zhang et al 2022 found 2% in a similar but non-identical
question). Many respondents were substantially more concerned: 48% of
respondents gave at least a 10% chance of an extremely bad outcome. But
some were much less concerned: 25% put it at 0%.</li>
<li><strong>The median respondent thinks there is an "about even chance"
that a stated argument for an intelligence explosion is broadly
correct.</strong> 54% of respondents say the likelihood that it is
correct is "about even," "likely," or "very likely" (corresponding to
probability &gt;40%), similar to 51% of respondents in 2016. The median
respondent also believes machine intelligence will probably (60%) be
"vastly better than humans at all professions" within 30 years of HLMI,
and the rate of global technological improvement will probably (80%)
dramatically increase (e.g., by a factor of ten) as a result of machine
intelligence within 30 years of HLMI.</li>
</ol>
<p>(Here, "HLMI" abbreviates "human-level machine intelligence". These
definitions can be hard to justify since there are no well-established
theories of intelligence. The usual conception of AGI includes HLMI i.e.
the "general" in AGI means "more general than (some sort of
reasonably-taken average) human intelligence". I find HLMI a much less
ambiguous term than AGI, so I prefer it, but it's also clear that AGI
has become the popular term of choice for this kind of concept
regardless of its original intentions.)</p>
<p>Imminent AI Doomsday predictions often cite the 37-year average
forecast time to a 50% chance of HLMI. But, note that the probability is
conditional on "human scientific activity continu[ing] without major
negative disruption", which of course I would not expect the surveyed
population to have any special insight into (in contrast to the topic of
AI technological development). Additionally, the response is subject to
selection biases:</p>
<ul>
<li><p>The population is self-selected to be excited about AI technology
since they decided to focus their career working on it, which implies
they rate the probability of achieving one of their main goals --
developing HLMI -- higher due to desirability bias</p></li>
<li><p>The population is self-selected to be less worried about AI
Doomsday since those that are very worried about AI Doomsday would be
less interested in contributing to it. Additionally, AI researchers are
incentivized to underrate the likelihood of AI Doomsday because a higher
perceived likelihood would probably lead to more regulation and other
inhibitions to their primary work and source of income.</p></li>
<li><p>The population is self-selected to rate the timeline to HLMI as
shorter and the likelihood of AI Doomsday as higher due to people with
those beliefs being more interested in a survey asking about those
relatively-niche topics. The survey only got a 17% response rate, which
of course was not a uniformly-random 17%. So, this bias could be quite
significant. The survey's Methods section's description of the surveyed
population:</p></li>
</ul>
<blockquote>
<p>We contacted approximately 4271 researchers who published at the
conferences NeurIPS or ICML in 2021. These people were selected by
taking all of the authors at those conferences and randomly allocating
them between this survey and a survey being run by others. We then
contacted those whose email addresses we could find. We found email
addresses in papers published at those conferences, in other public
data, and in records from our previous survey and Zhang et al 2022. We
received 738 responses, some partial, for a 17% response rate.
Participants who previously participated in the the 2016 ESPAI or Zhang
et al surveys received slightly longer surveys, and received questions
which they had received in past surveys (where random subsets of
questions were given), rather than receiving newly randomized questions.
This was so that they could also be included in a 'matched panel’
survey, in which we contacted all researchers who completed the 2016
ESPAI or Zhang et al surveys, to compare responses from exactly the same
samples of researchers over time. These surveys contained additional
questions matching some of those in the Zhang et al survey. - <em>Update
2023/04/09:</em> (As brought up to me by a friend) Most of the
researchers surveyed were from academia, and only a few of the
researchers surveyed were from the currently-leading AI companies. Since
those companies seem to have a clear lead over academic AI efforts, this
leads to a bias where an important class of researchers who are most
familiar with the most advanced existing AI technology is less
represented. It's hard to say exactly which way this bias goes. It could
imply a bias towards a longer timeline since the advanced industry
researchers know how much further the technology already is secretly
ahead of what the public knows. Or, it could imply a bias towards a
shorter timeline since the advanced industry researchers are more
familiar with the limitations of current approaches while academics with
fewer details might expect that the recent rates of rapid advancement
should be expected to continue. Overall, I'll summarize this as a net
negligible bias that increases uncertainty. - <em>Update
2023/04/30:</em> From OpenAI's CEO Sam Altman: "I think we're at the end
of the era where it's going to be these, like, giant, giant models,” he
told an audience at an event held at MIT late last week. "We'll make
them better in other ways." [<span><a
href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/"><img
src="/favicon/https%253A%252F%252Fwww.wired.com%252Fstory%252Fopenai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.wired.com%2Fstory%2Fopenai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over%2F.ico" />wired</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.wired.com%252Fstory%252Fopenai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.wired.com%2Fstory%2Fopenai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over%2F.ico" />https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/</a></em></span><span
class="preview-description">Placeholder description for
https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/</span></span></span>].
The immediate implication is that recent major advances have been
significantly driven by scaling existing techniques, and so since that
scaling's effectiveness is imminently dropping off, fundamentally new
techniques are required in order to make progress. This development is
evidence that, at the time of the survey, those in the industry, who
would be more likely to predict this situation since they are more
knowledgeable about the cutting edge, would be influenced by this to
predict longer timelines to HLMI than the surveyed academics who would
only be influenced by extrapolating current trends about industry AI
development from an external perspective.</p>
</blockquote>
<p>Unfortunately, the survey didn't ask for a predicted timeline within
which respondents expect the effect of advanced AI on humanity to be
"extremely bad (e.g. human extinction)".</p>
<p>There are many biases to account for, as with any survey, but it
seems there should be something like a general trend of: - significant
underestimation of the time-to-HLMI - slight overestimation of the risk
of (eventual) AI Doomsday - higher uncertainty due to the most advanced
researchers in industry not being included as much as academics</p>
<p>Additionally, the variance among survey responses is very high. There
is nothing like a clear consensus on the near term (50 years), as this
<span><a href="https://ourworldindata.org/ai-timelines"><img
src="/favicon/https%253A%252F%252Fourworldindata.org%252Fai-timelines.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fourworldindata.org%2Fai-timelines.ico" />post
by Our World In Data</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://ourworldindata.org/ai-timelines" title="_blank"><img
src="/favicon/https%253A%252F%252Fourworldindata.org%252Fai-timelines.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fourworldindata.org%2Fai-timelines.ico" />https://ourworldindata.org/ai-timelines</a></em></span><span
class="preview-description">Placeholder description for
https://ourworldindata.org/ai-timelines</span></span></span> mentions.
However, there is a relatively strong consensus (90% of respondents)
that HLMI will be developed within the next 100 years. However, 100
years is a very long timeline for any technological prediction, so I
take it as having much less certainty than a much more near-term
prediction like 37 years.</p>
<p>With that said, consider how these results compare to the popular
imminent AI Doomsday predictions.</p>
<h3 id="timeline-to-hlmi">Timeline to HLMI</h3>
<p>Imminent AI Doomsday predictions often rate the time-to-HLMI as even
shorter than this survey suggests and the likelihood of eventual
Doomsday as even higher. For example, Eliezer Yudkowsky -- one of the
main public imminent AI Doomsday predictors -- suggests in his <span><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"><img
src="/favicon/https%253A%252F%252Ftime.com%252F6266923%252Fai-eliezer-yudkowsky-open-letter-not-enough%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ftime.com%2F6266923%2Fai-eliezer-yudkowsky-open-letter-not-enough%2F.png" />TIME
letter</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"
title="_blank"><img
src="/favicon/https%253A%252F%252Ftime.com%252F6266923%252Fai-eliezer-yudkowsky-open-letter-not-enough%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ftime.com%2F6266923%2Fai-eliezer-yudkowsky-open-letter-not-enough%2F.png" />https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/</a></em></span><span
class="preview-description">Placeholder description for
https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/</span></span></span>
that imminent Doomsday is extremely likely unless drastic and extremely
unlikely regulations on AI development are imposed soon i.e. we "shut it
all down". For example, he has recommended in various discussions (e.g.
on <span><a href="https://overcast.fm/+eZyDo-AY0"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252BeZyDo-AY0.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2BeZyDo-AY0.ico" />Lex
Friedman's podcast</a><span class="sidenote preview"><span
class="preview-title"><em><a href="https://overcast.fm/+eZyDo-AY0"
title="_blank"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252BeZyDo-AY0.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2BeZyDo-AY0.ico" />https://overcast.fm/+eZyDo-AY0</a></em></span><span
class="preview-description">Placeholder description for
https://overcast.fm/+eZyDo-AY0</span></span></span>) that young people
today should not put much weight on the future because of how likely
imminent AI Doomsday is (perhaps this was rhetorical and he doesn't
truly believe that the timeline is that sure, but my impression is that
he has never intentionally been rhetorical in any kind of way similar to
this so I doubt that he is in this instance either).</p>
<p>However, Yudkowsky has repeatedly refused to give a specific
probability distribution over the likelihood of imminent AI Doomsday
timelines. He explained this strategy on <span><a
href="https://overcast.fm/+b53M_HRgo"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252Bb53M_HRgo.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2Bb53M_HRgo.ico" />The Lunar
Society podcast</a><span class="sidenote preview"><span
class="preview-title"><em><a href="https://overcast.fm/+b53M_HRgo"
title="_blank"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252Bb53M_HRgo.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2Bb53M_HRgo.ico" />https://overcast.fm/+b53M_HRgo</a></em></span><span
class="preview-description">Placeholder description for
https://overcast.fm/+b53M_HRgo</span></span></span>. My summary of it is
that he believes that publicly revealing specific predictions would be
misleading because: it's very difficult to predict the particular
pathway to imminent AI Doomsday, so his specific predictions would
likely be wrong (even if he correctly thought they were more likely than
people who do not predict imminent AI Doomsday), so people would
incorrectly infer that his prediction of imminent AI Doomsday is
unlikely even though the ultimate convergence of many individually
unlikely pathways is together very likely. Additionally, he doesn't
think there are any intermediate milestone predictions that he expects
to be convergent in the same way that people could use to accurately
judge his model that predicts AI Doomsday.</p>
<p>This is a coherent position to have. As he gives as an example,
predicting the lottery is also like this: it's very difficult to predict
<em>which</em> lottery ticket is the winning lottery ticket, but it's
very easy to predict that <em>some</em> winning ticket will be
drawn.</p>
<p>This position has as a premise that, before his ultimate prediction
of imminent AI Doomsday plays out (or perhaps until <em>right</em>
before imminent AI Doomsday), there are no intermediate predictions that
he could make that we could judge his model by. This implies that when
we observe certain events related to AI development that he doesn't
think immediately portent AI Doomsday, they should not impact his
predictions. Since if they did, then he could have made them as
intermediate predictions beforehand. However, as judged from his
comments on Twitter and in podcasts, it does seem like he's updating
regularly by judging particular AI behaviors (for example, Bing
threatening users with blackmail) to support his dire predictions.</p>
<p>My guess is that he's leaving a lot of credibility "on the table"
here; it would be easy for him to make near-term predictions about ways
that AI technology will act that most people would not expect. If he did
so, many people (including me) would take his concerns about eventual,
and to some extent, AI Doomsday more seriously.</p>
<p>In particular, he could bundle together in whatever way he likes the
classes of possible observations that would increase his expectation of
imminent AI Doomsday and predict that at least one of these things will
happen in some specified timeline. Even if each bundle item is unlikely,
he could find some disjunctive bundle that he thinks is somewhat close
to 50% likely and that some prominent people that disagree with him on
imminent AI Doomsday would put at much less than 50% likelihood. I hope
he or another imminent AI Doomsday predictor does something like
this.</p>
<p>&lt;!-- I appreciated the <span><a
href="https://overcast.fm/+b53M_HRgo"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252Bb53M_HRgo.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2Bb53M_HRgo.ico" />The Lunar
Society podcast</a><span class="sidenote preview"><span
class="preview-title"><em><a href="https://overcast.fm/+b53M_HRgo"
title="_blank"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252Bb53M_HRgo.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2Bb53M_HRgo.ico" />https://overcast.fm/+b53M_HRgo</a></em></span><span
class="preview-description">Placeholder description for
https://overcast.fm/+b53M_HRgo</span></span></span> with Yudkowsky more
than his appearance on <span><a
href="https://overcast.fm/+eZyDo-AY0"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252BeZyDo-AY0.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2BeZyDo-AY0.ico" />Lex
Friedman podcast</a><span class="sidenote preview"><span
class="preview-title"><em><a href="https://overcast.fm/+eZyDo-AY0"
title="_blank"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252BeZyDo-AY0.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2BeZyDo-AY0.ico" />https://overcast.fm/+eZyDo-AY0</a></em></span><span
class="preview-description">Placeholder description for
https://overcast.fm/+eZyDo-AY0</span></span></span>. I think Friedman
let Yudkowsky dwell on some conclusions too long without explaining the
details of how he arrived at those conclusions. I was surprised to
finish the hours-long podcast and not learn anything (as far as I could
tell) about what MIRI (the Machine Intelligence Research Institute that
Yudkowsky runs) has accomplished. Patel, on the other hand, presented
many more interesting lines thought for Yudkowsky to entertain, but I
think he still indulged Yudkowsky's premises a little too much without
asking for more details and justification e.g. why he is so much more
sure that AI Doomsday is likely to happen shortly (even if he doesn't
give any specific timeline) than most AI researchers (accounting for
biases of course). Even <span><a href="https://gwern.net"><img
src="/favicon/https%253A%252F%252Fgwern.net.png" class="favicon"
alt="/favicon/https%3A%2F%2Fgwern.net.png" />Gwern Branwen</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://gwern.net" title="_blank"><img
src="/favicon/https%253A%252F%252Fgwern.net.png" class="favicon"
alt="/favicon/https%3A%2F%2Fgwern.net.png" />https://gwern.net</a></em></span><span
class="preview-description">Placeholder description for
https://gwern.net</span></span></span>, who Yudkowsky brings up as a
good predictor for near-term AI developments, doesn't have Yudkowsky's
(and many others that concur with him) confidence in imminent AI
Doomsday. --&gt;</p>
<h3
id="most-ai-researchers-expect-that-superintelligent-ai-will-not-be-extremely-bad">Most
AI Researchers Expect that Superintelligent AI will <em>not</em> be
Extremely Bad</h3>
<p>In the survey, the aggregate forecast time to a 50% chance of HLMI is
37 years, the median correspondent things that there is an "about even
chance" that an intelligence explosion will happen (withing 30 years of
HLMI) and they believe that the probability that the long-run effect of
advanced AI on humanity will be "extremely bad (e.g. human extinction)"
with probability 5%. - Note that this is within 1% of Scott Alexander's
<span><a
href="https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/"><img
src="/favicon/https%253A%252F%252Fslatestarcodex.com%252F2013%252F04%252F12%252Fnoisy-poll-results-and-reptilian-muslim-climatologists-from-mars%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fslatestarcodex.com%2F2013%2F04%2F12%2Fnoisy-poll-results-and-reptilian-muslim-climatologists-from-mars%2F.ico" />lizardman's
constant</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/"
title="_blank"><img
src="/favicon/https%253A%252F%252Fslatestarcodex.com%252F2013%252F04%252F12%252Fnoisy-poll-results-and-reptilian-muslim-climatologists-from-mars%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fslatestarcodex.com%2F2013%2F04%2F12%2Fnoisy-poll-results-and-reptilian-muslim-climatologists-from-mars%2F.ico" />https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/</a></em></span><span
class="preview-description">Placeholder description for
https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/</span></span></span>,
which is the observation that for most surveys, for responses that
indicate very unorthodox and unjustified beliefs, at least 4% of
respondents will give those responses (regardless of honesty).</p>
<p>With some slight extrapolation, the median correspondent believes
something close to this: it's fairly likely (~25%) that superintelligent
AI will be developed within ~67 years (50% chance of HLMI in 37 years,
then allowing for up to 30 years for the 50% chance of intelligence
explosion to play out), yet they also believe that the long-run effect
of advanced AI on humanity is likely (95%) not extremely bad.</p>
<p>This doesn't correspond well to the usual imminent AI Doomday
position, which predicts that superintelligent AI is <em>extremely
likely</em> to cause AI Doomsday. If an imminent AI Doomsday predictor
wants to use this data to support their claim about the short timeline,
they should also take into account, or adequately explain why they are
not, the aggregate position of the respondents that they aren't nearly
as worried about AI Doomsday once superintelligence is developed.</p>
<h3 id="my-current-position-on-imminent-ai-doomsday">My Current Position
on Imminent AI Doomsday</h3>
<p>Almost all the arguments I've heard from imminent AI Doomsday
predictors focus almost on supporting the <em>eventual</em> AI Doomsday
argument and neglect or hand-wave the implications for <em>imminent</em>
AI Doomsday. Of course, the support for eventual AI Doomsday is
<em>some</em> support for imminent AI Doomsday, but not much.</p>
<p>For example, Tyler Cowen wrote a <span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fexistential-risk-and-the-turn-in-human-history.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fexistential-risk-and-the-turn-in-human-history.html.png" />spirited
post</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"
title="_blank"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fexistential-risk-and-the-turn-in-human-history.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fexistential-risk-and-the-turn-in-human-history.html.png" />https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html</a></em></span><span
class="preview-description">Placeholder description for
https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html</span></span></span>
claiming among other things that the imminent AI Doomsday predictors are
much too confident. Scott Alexander <span><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fastralcodexten%252Fp%252Fmr-tries-the-safe-uncertainty-fallacy.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fastralcodexten%2Fp%2Fmr-tries-the-safe-uncertainty-fallacy.ico" />responded
to Cowen</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"
title="_blank"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fastralcodexten%252Fp%252Fmr-tries-the-safe-uncertainty-fallacy.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fastralcodexten%2Fp%2Fmr-tries-the-safe-uncertainty-fallacy.ico" />https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</a></em></span><span
class="preview-description">Placeholder description for
https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</span></span></span>
with the claim that Cowen's argument was weak because it suffered from
the "safe uncertainty fallacy" -- that is, if the specifics of a
potential danger are not clear that does <em>not</em> necessarily imply
that the dangers are less likely. Alexander analogizes the situation to
discovering that an alien spaceship is heading towards Earth -- we don't
know anything about the aliens, so any particular predictions about what
will happen when they get here are uncertain, and yet we definitely
should be worried about the potential dangers because aliens are likely
enough to be dangerous enough in <em>some</em> way to warrant worry. I
think Alexander missed that point of Cowen's article, which it seems
that <span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fthursday-assorted-links-398.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fthursday-assorted-links-398.html.png" />Cowen
agrees with me about</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"
title="_blank"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fthursday-assorted-links-398.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fthursday-assorted-links-398.html.png" />https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html</a></em></span><span
class="preview-description">Placeholder description for
https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html</span></span></span>,
because Alexander's argument starts with the premise that we already
<em>know</em> that a big potential danger is on the horizon, such as the
alien spaceship. But with AI, Cowen <em>explicitly disagrees</em> that
AI poses such an immediate danger. Alexander probably does disagree with
Cowen about this, but he doesn't present an argument for this premise in
his article and so it doesn't make sense to argue against Cowen by first
assuming he is wrong.</p>
<p>However, I'm sure there are some good arguments for imminent AI
Doomsday out there, so I'll look for them.</p>
<p>As I discussed above, I do in fact believe in eventual AI Doomsday.
But my base rate expectation for something like that happening within
the next 20 years is very low. Before the most recent impressive public
advances in AI technology, ChatGPT and other <span><a
href="https://en.wikipedia.org/wiki/Large_language_model"><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FLarge_language_model.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLarge_language_model.ico" />large
language models (LLM)</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://en.wikipedia.org/wiki/Large_language_model"
title="_blank"><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FLarge_language_model.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLarge_language_model.ico" />https://en.wikipedia.org/wiki/Large_language_model</a></em></span><span
class="preview-description">Placeholder description for
https://en.wikipedia.org/wiki/Large_language_model</span></span></span>
in particular, my expectation for HLMI in the next 20 years was that it
was negligible, something like 1%. Nothing I'd seen had indicated that
state-of-the-art AI technology was on track for anything like that
without many breakthroughs. And, the field has been in a lot of flux in
the last decade or two to say the least, which indicates that solid,
compounding progress had not kicked in yet -- which is something
important that I'd expect to see before the development run up to
HLMI.</p>
<p>I was very impressed by ChatGPT, using it for myself and seeing other
people online share what they could do with it. What impressed me was
the breadth of interesting things that could be accomplished by such a
fundamentally theory-lacking approach. The model powering ChatGPT is a
<em>huge</em> version of basically the same kind of model that has been
used over the last 5 years or so (the generative pre trained transformer
(GPT) was <span><a href="https://arxiv.org/abs/1706.03762"><img
src="/favicon/https%253A%252F%252Farxiv.org%252Fabs%252F1706.03762.png"
class="favicon"
alt="/favicon/https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762.png" />introduced
by Google Brain in 2017</a><span class="sidenote preview"><span
class="preview-title"><em><a href="https://arxiv.org/abs/1706.03762"
title="_blank"><img
src="/favicon/https%253A%252F%252Farxiv.org%252Fabs%252F1706.03762.png"
class="favicon"
alt="/favicon/https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762.png" />https://arxiv.org/abs/1706.03762</a></em></span><span
class="preview-description">Placeholder description for
https://arxiv.org/abs/1706.03762</span></span></span>), plus some
human-powered fine-tuning (i.e. <span><a
href="https://huggingface.co/blog/rlhf"><img
src="/favicon/https%253A%252F%252Fhuggingface.co%252Fblog%252Frlhf.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fhuggingface.co%2Fblog%2Frlhf.ico" />reinforcement
learning from human feedback (RLHF)</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://huggingface.co/blog/rlhf" title="_blank"><img
src="/favicon/https%253A%252F%252Fhuggingface.co%252Fblog%252Frlhf.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fhuggingface.co%2Fblog%2Frlhf.ico" />https://huggingface.co/blog/rlhf</a></em></span><span
class="preview-description">Placeholder description for
https://huggingface.co/blog/rlhf</span></span></span>). I only found
ChatGPT to be a minor step towards HLMI, if nothing else it shows that
there are still meaningful gains to be had in scaling current
technologies. It has become a sort of meme, among imminent AI Doomsday
predictors in particular, to make fun of people who claim that ChatGPT
is just a "stochastic parrot", usually by presenting a ChatGPT
transcript and claiming its obviously "truly thinking" or something like
that. Yet, it still displays many telltale signs of whatever "stochastic
parrot" probably is, in that if you give it challenging prompts that are
unlike things from its training set, it reliably fails; an AI system
that was effectively generalizing and not just doing relatively
low-order pattern matching. For example: - ChatGPT can do a lot of
simple math problems, but if you expand a math problem it can normally
do ok to one that has a lot of complications and isn't just a
straightforward textbook problem then it struggles -- it will give you
<em>some</em> answers but with serious flaws - ChatGPT can do some
standard logical derivations, but cannot reason about a non-standard
logical system. If I present a new logical system, even including a
variety of examples of derivations in the new system, ChatGPT forgets
about the rules I specified and uses whatever standard logical rules
happen to have a similar form instead.</p>
<p>The step from ChatGPT to GPT4 was more subtle in terms of superficial
user experience. But, personally, my impression was that it responded
appropriately to my exact intentions <em>much</em> more often and
precisely than ChatGPT, especially in terms of respecting specific
details of my requests and giving more detail-oriented rather than very
general unassuming responses. For example: - GPT4 can write a short
story that weaves together a large number of specified elements without
forgetting any of them or hyper focussing on only a few.</p>
<p>In terms of distances from HLMI, GPT4 is the same order of magnitude
distances from it as ChatGPT i.e. I don't consider it a much larger step
towards HLMI than ChatGPT was. But, it was still surprising to me that
such a large improvement could be made in such short order. So overall
given these developments within the last year, I respectfully increase
my expectation that HLMI will happen in 20 years to 2%.</p>
<p><em>Update 2023/04/30:</em> I've since learned that GPT4 was actually
not developed so quickly after GPT3.5 as I previously thought. OpenAI's
Sam Altman stated that "we have had the initial training of GPT-4 done
for quite awhile, but it’s taken us a long time and a lot of work to
feel ready to release it." [<span><a
href="https://twitter.com/sama/status/1635687859494715393?s=20"><img
src="/favicon/https%253A%252F%252Ftwitter.com%252Fsama%252Fstatus%252F1635687859494715393%253Fs%253D20.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Ftwitter.com%2Fsama%2Fstatus%2F1635687859494715393%3Fs%3D20.ico" />twitter</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://twitter.com/sama/status/1635687859494715393?s=20"
title="_blank"><img
src="/favicon/https%253A%252F%252Ftwitter.com%252Fsama%252Fstatus%252F1635687859494715393%253Fs%253D20.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Ftwitter.com%2Fsama%2Fstatus%2F1635687859494715393%3Fs%3D20.ico" />https://twitter.com/sama/status/1635687859494715393?s=20</a></em></span><span
class="preview-description">Placeholder description for
https://twitter.com/sama/status/1635687859494715393?s=20</span></span></span>].
I haven't been able to find more specific information generally
available online, but I've heard from Altman on podcast interviews that
GPT4's "initial training" was done around the time of the release of
ChatGPT (2022/11/30). So, the improvement from GPT3.5 to GPT4 wasn't
actually done as quickly as I previously thought, but that GPT4-level
capabilities were mostly already accomplished by the time that GPT3.5
was available. This implies that its harder to judge how quickly
capabilities improved from GPT3.5-level to GPT4-level.</p>
<h3 id="what-i-would-consider-a-major-shortcoming-in-my-reasoning">What
I Would Consider a Major Shortcoming in My Reasoning</h3>
<p>A common argument for imminent AI Doomsday that I haven't addressed
is the "fast takeoff" or "foom" scenario: a certain level of AI
advancement that is achievable in the near-term future could lead to an
extremely quick feedback loop of self-improving AI systems, which within
a very short period advances to AI to superintelligence.</p>
<p>This sort of scenario is difficult to handle because it has very
little warning -- we might not know much about how likely it is to
happen until it's right about to or already happening. If it's possible
to solidly predict a non-negligible probability of fast-takeoff, I think
that's one of the most important things that imminent Doomsday
predictors could do to advance their case.</p>
<p>Abstracting away from the fast takeoff question, I find the main
feature lacking among imminent Doomsday arguments is detailed to support
specific probabilities. In-principle arguments for the in-principle
possibility and so <em>eventual</em> likelihood of AI Doomsday don't say
much about <em>imminent</em> Doomsday. If I was presented with a
compelling case for why specific features of AI systems can be
demonstrated to very clearly be extrapolated to follow a widely agreed
upon path towards superintelligence in the near term, I would be utterly
convinced. The fact that AI Doomsday predictors don't seem to spend much
effort trying to present specific ways in which current AI systems could
be extrapolated in this way, and instead focus on very unquantifiable
feelings about how scary and/or impressive some of the behaviors of AI
systems are, makes me skeptical. Notably, the researchers spending so
much time, effort, and money developing these systems haven't noticed
such an extrapolated pathway either. Given all these factors, and my
prior expectation that HLMI is very difficult, I think it's very
unlikely to exist.</p>
<p>Aside from this, advances in AI technology could also change my
mind.</p>
<p>&lt;!-- But, given that there is still a lot of room left between
current systems and superintelligent AI (or even HLMI), --&gt;</p>
<h3 id="what-i-would-consider-significant-steps-towards-hlmi">What I
Would Consider Significant Steps Towards HLMI</h3>
<p><em>An underlying theory.</em> I would consider it a major step
towards HLMI if the field of AI research establishes some
widely-respected new theories about the nature of intelligence, probably
including the intelligence of animals at the least. Until then, I expect
more lumpy developments like the explosive rise of AI in public
consciousness recently as surprisingly effective techniques are stumbled
into. But my prior is that HLMI is so difficult that it will probably
not be "stumbled into" in this way.</p>
<p><em>Generality.</em> I would consider it a major step towards HLMI if
I can specify an arbitrary logical system (<em>especially</em> one that
is very unlike a standard logical system) and the AI system can reason
about it with relatively perfect accuracy (&gt;95%). This would
demonstrate that the system has fully encoded logical reasoning, and is
ready to replace math grad students (haha). In all seriousness, this
would be a critical development because it implies that you can give a
specification (to some degree informal, of course) of your desires and
have some high-level but still mathematically-formalizable expectations
about what the AI will do with it. Effectively abstracting the behavior
of components of systems is the foundation of scaling. Once you can
reliably break up tasks into pieces that the AI can perform up to some
spec, then you can compose them together and abstract hierarchies of
specifications that will allow for the design of organizations of AI
systems that mimic the kind of powerful abstract organization in
programming and society. Whole "governments" of AI systems could operate
autonomously with predictable behavior up to some requirements. This is
already <span><a href="https://arxiv.org/abs/2303.17580"><img
src="/favicon/https%253A%252F%252Farxiv.org%252Fabs%252F2303.17580.png"
class="favicon"
alt="/favicon/https%3A%2F%2Farxiv.org%2Fabs%2F2303.17580.png" />possible
to some extent</a><span class="sidenote preview"><span
class="preview-title"><em><a href="https://arxiv.org/abs/2303.17580"
title="_blank"><img
src="/favicon/https%253A%252F%252Farxiv.org%252Fabs%252F2303.17580.png"
class="favicon"
alt="/favicon/https%3A%2F%2Farxiv.org%2Fabs%2F2303.17580.png" />https://arxiv.org/abs/2303.17580</a></em></span><span
class="preview-description">Placeholder description for
https://arxiv.org/abs/2303.17580</span></span></span>, but with
significant inherent limitations on abstraction and delegation.</p>
<h2
id="how-hard-is-it-to-avoid-ai-doomsday-aka-the-alignment-problem">How
Hard Is It To Avoid AI Doomsday? (aka The Alignment Problem)</h2>
<p>Seems very hard.</p>
<p>I haven't given this question <em>as</em> much thought; I believe
that imminent AI Doomsday is unlikely, so I think we still have plenty
of time to consider it. Additionally, I expect that the details of AI
technology will change significantly in ways relevant to this question
before AI Doomsday <em>does</em> become imminent, so there's probably
not much progress we can make on answering this question until we have
much more advanced and understood AI technology.</p>
<h2 id="more-resource">More Resource</h2>
<p>Below are some media on this topic that I recommend. One of the main
reasons why I wanted to write this post is because much of the
discussion of this topic I've seen seems very confused (or -- confusing
to me perhaps), and so I want to put down my own thoughts so that I can
get other people's pointed feedback and clearly track how my opinions
change.</p>
<ul>
<li><span><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"><img
src="/favicon/https%253A%252F%252Faiimpacts.org%252F2022-expert-survey-on-progress-in-ai%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Faiimpacts.org%2F2022-expert-survey-on-progress-in-ai%2F.ico" />AiImpacts.org
-- 2022 Expert Survey on Progress in AI</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
title="_blank"><img
src="/favicon/https%253A%252F%252Faiimpacts.org%252F2022-expert-survey-on-progress-in-ai%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Faiimpacts.org%2F2022-expert-survey-on-progress-in-ai%2F.ico" />https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</a></em></span><span
class="preview-description">Placeholder description for
https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/</span></span></span></li>
<li><span><a
href="https://open.substack.com/pub/overcomingbias/p/ai-risk-again"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fovercomingbias%252Fp%252Fai-risk-again.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fovercomingbias%2Fp%2Fai-risk-again.ico" />Robin
Hanson -- current views on AI danger</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://open.substack.com/pub/overcomingbias/p/ai-risk-again"
title="_blank"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fovercomingbias%252Fp%252Fai-risk-again.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fovercomingbias%2Fp%2Fai-risk-again.ico" />https://open.substack.com/pub/overcomingbias/p/ai-risk-again</a></em></span><span
class="preview-description">Placeholder description for
https://open.substack.com/pub/overcomingbias/p/ai-risk-again</span></span></span></li>
<li><span><a
href="https://open.substack.com/pub/erikhoel/p/how-to-navigate-the-ai-apocalypse"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Ferikhoel%252Fp%252Fhow-to-navigate-the-ai-apocalypse.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Ferikhoel%2Fp%2Fhow-to-navigate-the-ai-apocalypse.ico" />Erik
Hoel -- how to navigate the AI apocalypse</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://open.substack.com/pub/erikhoel/p/how-to-navigate-the-ai-apocalypse"
title="_blank"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Ferikhoel%252Fp%252Fhow-to-navigate-the-ai-apocalypse.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Ferikhoel%2Fp%2Fhow-to-navigate-the-ai-apocalypse.ico" />https://open.substack.com/pub/erikhoel/p/how-to-navigate-the-ai-apocalypse</a></em></span><span
class="preview-description">Placeholder description for
https://open.substack.com/pub/erikhoel/p/how-to-navigate-the-ai-apocalypse</span></span></span></li>
<li><span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fexistential-risk-and-the-turn-in-human-history.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fexistential-risk-and-the-turn-in-human-history.html.png" />Tyler
Cowen -- Existential risk, AI, and the inevitable turn in human
history</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"
title="_blank"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fexistential-risk-and-the-turn-in-human-history.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fexistential-risk-and-the-turn-in-human-history.html.png" />https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html</a></em></span><span
class="preview-description">Placeholder description for
https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html</span></span></span></li>
<li><span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fthe-permanent-pause.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fthe-permanent-pause.html.png" />Tyler
Cowen -- response to FLI letter</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html"
title="_blank"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fthe-permanent-pause.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fthe-permanent-pause.html.png" />https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html</a></em></span><span
class="preview-description">Placeholder description for
https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html</span></span></span></li>
<li><span><a
href="https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves"><img
src="/favicon/https%253A%252F%252Fwww.bloomberg.com%252Fopinion%252Farticles%252F2023-04-03%252Fshould-we-pause-ai-we-d-only-be-hurting-ourselves.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.bloomberg.com%2Fopinion%2Farticles%2F2023-04-03%2Fshould-we-pause-ai-we-d-only-be-hurting-ourselves.png" />Tyler
Cowen -- against pausing AI development</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves"
title="_blank"><img
src="/favicon/https%253A%252F%252Fwww.bloomberg.com%252Fopinion%252Farticles%252F2023-04-03%252Fshould-we-pause-ai-we-d-only-be-hurting-ourselves.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.bloomberg.com%2Fopinion%2Farticles%2F2023-04-03%2Fshould-we-pause-ai-we-d-only-be-hurting-ourselves.png" />https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves</a></em></span><span
class="preview-description">Placeholder description for
https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves</span></span></span></li>
<li><span><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fthursday-assorted-links-398.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fthursday-assorted-links-398.html.png" />Tyler
Cowen -- response to ACX's response</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"
title="_blank"><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fthursday-assorted-links-398.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fthursday-assorted-links-398.html.png" />https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html</a></em></span><span
class="preview-description">Placeholder description for
https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html</span></span></span></li>
<li><span><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"><img
src="/favicon/https%253A%252F%252Ftime.com%252F6266923%252Fai-eliezer-yudkowsky-open-letter-not-enough%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ftime.com%2F6266923%2Fai-eliezer-yudkowsky-open-letter-not-enough%2F.png" />Eliezer
Yudkowsky -- (response to FLI letter) Pausing AI Developments Isn't
Enough. We Need to Shut it All Down</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"
title="_blank"><img
src="/favicon/https%253A%252F%252Ftime.com%252F6266923%252Fai-eliezer-yudkowsky-open-letter-not-enough%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ftime.com%2F6266923%2Fai-eliezer-yudkowsky-open-letter-not-enough%2F.png" />https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/</a></em></span><span
class="preview-description">Placeholder description for
https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/</span></span></span></li>
<li><span><a href="https://overcast.fm/+eZyDo-AY0"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252BeZyDo-AY0.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2BeZyDo-AY0.ico" />Eliezer
Yudkowsky -- Lex Friedman podcast</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://overcast.fm/+eZyDo-AY0" title="_blank"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252BeZyDo-AY0.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2BeZyDo-AY0.ico" />https://overcast.fm/+eZyDo-AY0</a></em></span><span
class="preview-description">Placeholder description for
https://overcast.fm/+eZyDo-AY0</span></span></span></li>
<li><span><a href="https://overcast.fm/+b53M_HRgo"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252Bb53M_HRgo.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2Bb53M_HRgo.ico" />Eliezer
Yudkowsky -- Lunar Society podcast</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://overcast.fm/+b53M_HRgo" title="_blank"><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252Bb53M_HRgo.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2Bb53M_HRgo.ico" />https://overcast.fm/+b53M_HRgo</a></em></span><span
class="preview-description">Placeholder description for
https://overcast.fm/+b53M_HRgo</span></span></span></li>
<li><span><a
href="https://open.substack.com/pub/astralcodexten/p/why-i-am-not-as-much-of-a-doomer"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fastralcodexten%252Fp%252Fwhy-i-am-not-as-much-of-a-doomer.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fastralcodexten%2Fp%2Fwhy-i-am-not-as-much-of-a-doomer.ico" />ACX
-- predictions of AI apocalypse</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://open.substack.com/pub/astralcodexten/p/why-i-am-not-as-much-of-a-doomer"
title="_blank"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fastralcodexten%252Fp%252Fwhy-i-am-not-as-much-of-a-doomer.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fastralcodexten%2Fp%2Fwhy-i-am-not-as-much-of-a-doomer.ico" />https://open.substack.com/pub/astralcodexten/p/why-i-am-not-as-much-of-a-doomer</a></em></span><span
class="preview-description">Placeholder description for
https://open.substack.com/pub/astralcodexten/p/why-i-am-not-as-much-of-a-doomer</span></span></span></li>
<li><span><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fastralcodexten%252Fp%252Fmr-tries-the-safe-uncertainty-fallacy.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fastralcodexten%2Fp%2Fmr-tries-the-safe-uncertainty-fallacy.ico" />ACX
-- response to Tyler Cowen</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"
title="_blank"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fastralcodexten%252Fp%252Fmr-tries-the-safe-uncertainty-fallacy.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fastralcodexten%2Fp%2Fmr-tries-the-safe-uncertainty-fallacy.ico" />https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</a></em></span><span
class="preview-description">Placeholder description for
https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy</span></span></span></li>
<li><span><a
href="https://open.substack.com/pub/thezvi/p/response-to-tyler-cowens-existential"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fthezvi%252Fp%252Fresponse-to-tyler-cowens-existential.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fthezvi%2Fp%2Fresponse-to-tyler-cowens-existential.ico" />Zvi
-- response to Tyler Cowen</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://open.substack.com/pub/thezvi/p/response-to-tyler-cowens-existential"
title="_blank"><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fthezvi%252Fp%252Fresponse-to-tyler-cowens-existential.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fthezvi%2Fp%2Fresponse-to-tyler-cowens-existential.ico" />https://open.substack.com/pub/thezvi/p/response-to-tyler-cowens-existential</a></em></span><span
class="preview-description">Placeholder description for
https://open.substack.com/pub/thezvi/p/response-to-tyler-cowens-existential</span></span></span></li>
<li><span><a
href="https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter"><img
src="/favicon/https%253A%252F%252Fthezvi.substack.com%252Fp%252Fon-the-fli-ai-risk-open-letter.svg"
class="favicon"
alt="/favicon/https%3A%2F%2Fthezvi.substack.com%2Fp%2Fon-the-fli-ai-risk-open-letter.svg" />Zvi
-- reponse to FLI letter</a><span class="sidenote preview"><span
class="preview-title"><em><a
href="https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter"
title="_blank"><img
src="/favicon/https%253A%252F%252Fthezvi.substack.com%252Fp%252Fon-the-fli-ai-risk-open-letter.svg"
class="favicon"
alt="/favicon/https%3A%2F%2Fthezvi.substack.com%2Fp%2Fon-the-fli-ai-risk-open-letter.svg" />https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter</a></em></span><span
class="preview-description">Placeholder description for
https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter</span></span></span></li>
<li><span><a
href="https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time"><img
src="/favicon/https%253A%252F%252Fthezvi.substack.com%252Fp%252Feliezer-yudkowskys-letter-in-time.svg"
class="favicon"
alt="/favicon/https%3A%2F%2Fthezvi.substack.com%2Fp%2Feliezer-yudkowskys-letter-in-time.svg" />Zvi
-- response to Eliezer Yudkowsky's TIME article</a><span
class="sidenote preview"><span class="preview-title"><em><a
href="https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time"
title="_blank"><img
src="/favicon/https%253A%252F%252Fthezvi.substack.com%252Fp%252Feliezer-yudkowskys-letter-in-time.svg"
class="favicon"
alt="/favicon/https%3A%2F%2Fthezvi.substack.com%2Fp%2Feliezer-yudkowskys-letter-in-time.svg" />https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time</a></em></span><span
class="preview-description">Placeholder description for
https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time</span></span></span></li>
</ul>
<h1 id="references">References</h1>
<ul>
<li><a
href="https://thezvi.substack.com/p/eliezer-yudkowskys-letter-in-time"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fthezvi.substack.com%252Fp%252Feliezer-yudkowskys-letter-in-time.svg"
class="favicon"
alt="/favicon/https%3A%2F%2Fthezvi.substack.com%2Fp%2Feliezer-yudkowskys-letter-in-time.svg" />Zvi
-- response to Eliezer Yudkowsky's TIME article</a></li>
<li><a
href="https://thezvi.substack.com/p/on-the-fli-ai-risk-open-letter"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fthezvi.substack.com%252Fp%252Fon-the-fli-ai-risk-open-letter.svg"
class="favicon"
alt="/favicon/https%3A%2F%2Fthezvi.substack.com%2Fp%2Fon-the-fli-ai-risk-open-letter.svg" />Zvi
-- reponse to FLI letter</a></li>
<li><a
href="https://open.substack.com/pub/thezvi/p/response-to-tyler-cowens-existential"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fthezvi%252Fp%252Fresponse-to-tyler-cowens-existential.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fthezvi%2Fp%2Fresponse-to-tyler-cowens-existential.ico" />Zvi
-- response to Tyler Cowen</a></li>
<li><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fastralcodexten%252Fp%252Fmr-tries-the-safe-uncertainty-fallacy.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fastralcodexten%2Fp%2Fmr-tries-the-safe-uncertainty-fallacy.ico" />ACX
-- response to Tyler Cowen</a></li>
<li><a
href="https://open.substack.com/pub/astralcodexten/p/why-i-am-not-as-much-of-a-doomer"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fastralcodexten%252Fp%252Fwhy-i-am-not-as-much-of-a-doomer.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fastralcodexten%2Fp%2Fwhy-i-am-not-as-much-of-a-doomer.ico" />ACX
-- predictions of AI apocalypse</a></li>
<li><a href="https://overcast.fm/+b53M_HRgo" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252Bb53M_HRgo.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2Bb53M_HRgo.ico" />Eliezer
Yudkowsky -- Lunar Society podcast</a></li>
<li><a href="https://overcast.fm/+eZyDo-AY0" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252BeZyDo-AY0.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2BeZyDo-AY0.ico" />Eliezer
Yudkowsky -- Lex Friedman podcast</a></li>
<li><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Ftime.com%252F6266923%252Fai-eliezer-yudkowsky-open-letter-not-enough%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ftime.com%2F6266923%2Fai-eliezer-yudkowsky-open-letter-not-enough%2F.png" />Eliezer
Yudkowsky -- (response to FLI letter) Pausing AI Developments Isn't
Enough. We Need to Shut it All Down</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fthursday-assorted-links-398.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fthursday-assorted-links-398.html.png" />Tyler
Cowen -- response to ACX's response</a></li>
<li><a
href="https://www.bloomberg.com/opinion/articles/2023-04-03/should-we-pause-ai-we-d-only-be-hurting-ourselves"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.bloomberg.com%252Fopinion%252Farticles%252F2023-04-03%252Fshould-we-pause-ai-we-d-only-be-hurting-ourselves.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.bloomberg.com%2Fopinion%2Farticles%2F2023-04-03%2Fshould-we-pause-ai-we-d-only-be-hurting-ourselves.png" />Tyler
Cowen -- against pausing AI development</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fthe-permanent-pause.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fthe-permanent-pause.html.png" />Tyler
Cowen -- response to FLI letter</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fexistential-risk-and-the-turn-in-human-history.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fexistential-risk-and-the-turn-in-human-history.html.png" />Tyler
Cowen -- Existential risk, AI, and the inevitable turn in human
history</a></li>
<li><a
href="https://open.substack.com/pub/erikhoel/p/how-to-navigate-the-ai-apocalypse"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Ferikhoel%252Fp%252Fhow-to-navigate-the-ai-apocalypse.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Ferikhoel%2Fp%2Fhow-to-navigate-the-ai-apocalypse.ico" />Erik
Hoel -- how to navigate the AI apocalypse</a></li>
<li><a
href="https://open.substack.com/pub/overcomingbias/p/ai-risk-again"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fovercomingbias%252Fp%252Fai-risk-again.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fovercomingbias%2Fp%2Fai-risk-again.ico" />Robin
Hanson -- current views on AI danger</a></li>
<li><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Faiimpacts.org%252F2022-expert-survey-on-progress-in-ai%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Faiimpacts.org%2F2022-expert-survey-on-progress-in-ai%2F.ico" />AiImpacts.org
-- 2022 Expert Survey on Progress in AI</a></li>
<li><a href="https://arxiv.org/abs/2303.17580" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Farxiv.org%252Fabs%252F2303.17580.png"
class="favicon"
alt="/favicon/https%3A%2F%2Farxiv.org%2Fabs%2F2303.17580.png" />possible
to some extent</a></li>
<li><a href="https://twitter.com/sama/status/1635687859494715393?s=20"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Ftwitter.com%252Fsama%252Fstatus%252F1635687859494715393%253Fs%253D20.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Ftwitter.com%2Fsama%2Fstatus%2F1635687859494715393%3Fs%3D20.ico" />twitter</a></li>
<li><a href="https://huggingface.co/blog/rlhf" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fhuggingface.co%252Fblog%252Frlhf.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fhuggingface.co%2Fblog%2Frlhf.ico" />reinforcement
learning from human feedback (RLHF)</a></li>
<li><a href="https://arxiv.org/abs/1706.03762" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Farxiv.org%252Fabs%252F1706.03762.png"
class="favicon"
alt="/favicon/https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762.png" />introduced
by Google Brain in 2017</a></li>
<li><a href="https://en.wikipedia.org/wiki/Large_language_model"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FLarge_language_model.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLarge_language_model.ico" />large
language models (LLM)</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/thursday-assorted-links-398.html"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fthursday-assorted-links-398.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fthursday-assorted-links-398.html.png" />Cowen
agrees with me about</a></li>
<li><a
href="https://open.substack.com/pub/astralcodexten/p/mr-tries-the-safe-uncertainty-fallacy"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fopen.substack.com%252Fpub%252Fastralcodexten%252Fp%252Fmr-tries-the-safe-uncertainty-fallacy.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fopen.substack.com%2Fpub%2Fastralcodexten%2Fp%2Fmr-tries-the-safe-uncertainty-fallacy.ico" />responded
to Cowen</a></li>
<li><a
href="https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fmarginalrevolution.com%252Fmarginalrevolution%252F2023%252F03%252Fexistential-risk-and-the-turn-in-human-history.html.png"
class="favicon"
alt="/favicon/https%3A%2F%2Fmarginalrevolution.com%2Fmarginalrevolution%2F2023%2F03%2Fexistential-risk-and-the-turn-in-human-history.html.png" />spirited
post</a></li>
<li><a
href="https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fslatestarcodex.com%252F2013%252F04%252F12%252Fnoisy-poll-results-and-reptilian-muslim-climatologists-from-mars%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fslatestarcodex.com%2F2013%2F04%2F12%2Fnoisy-poll-results-and-reptilian-muslim-climatologists-from-mars%2F.ico" />lizardman's
constant</a></li>
<li><a href="https://gwern.net" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fgwern.net.png" class="favicon"
alt="/favicon/https%3A%2F%2Fgwern.net.png" />Gwern Branwen</a></li>
<li><a href="https://overcast.fm/+eZyDo-AY0" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252BeZyDo-AY0.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2BeZyDo-AY0.ico" />Lex
Friedman podcast</a></li>
<li><a href="https://overcast.fm/+b53M_HRgo" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252Bb53M_HRgo.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2Bb53M_HRgo.ico" />The Lunar
Society podcast</a></li>
<li><a href="https://overcast.fm/+b53M_HRgo" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252Bb53M_HRgo.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2Bb53M_HRgo.ico" />The Lunar
Society podcast</a></li>
<li><a href="https://overcast.fm/+eZyDo-AY0" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fovercast.fm%252F%252BeZyDo-AY0.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fovercast.fm%2F%2BeZyDo-AY0.ico" />Lex
Friedman's podcast</a></li>
<li><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Ftime.com%252F6266923%252Fai-eliezer-yudkowsky-open-letter-not-enough%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ftime.com%2F6266923%2Fai-eliezer-yudkowsky-open-letter-not-enough%2F.png" />TIME
letter</a></li>
<li><a href="https://ourworldindata.org/ai-timelines"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fourworldindata.org%252Fai-timelines.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fourworldindata.org%2Fai-timelines.ico" />post
by Our World In Data</a></li>
<li><a
href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.wired.com%252Fstory%252Fopenai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.wired.com%2Fstory%2Fopenai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over%2F.ico" />wired</a></li>
<li><a
href="https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Faiimpacts.org%252F2016-expert-survey-on-progress-in-ai%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Faiimpacts.org%2F2016-expert-survey-on-progress-in-ai%2F.ico" />2016
version of this survey</a></li>
<li><a
href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Faiimpacts.org%252F2022-expert-survey-on-progress-in-ai%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Faiimpacts.org%2F2022-expert-survey-on-progress-in-ai%2F.ico" />2022
Expert Survey on Progress in AI</a></li>
<li><a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FEliezer_Yudkowsky.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FEliezer_Yudkowsky.ico" />Eliezer
Yudkowsky</a></li>
<li><a
href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FSuperintelligence%253A_Paths%252C_Dangers%252C_Strategies.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSuperintelligence%3A_Paths%2C_Dangers%2C_Strategies.ico" />Superintelligence</a></li>
<li><a href="https://en.wikipedia.org/wiki/Nick_Bostrom"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fen.wikipedia.org%252Fwiki%252FNick_Bostrom.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FNick_Bostrom.ico" />Nick
Bostrom</a></li>
<li><a
href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Fwww.reuters.com%252Ftechnology%252Fchatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01%252F.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Fwww.reuters.com%2Ftechnology%2Fchatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01%2F.ico" />reuters</a></li>
<li><a
href="https://futureoflife.org/ai/faqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Ffutureoflife.org%252Fai%252Ffaqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ffutureoflife.org%2Fai%2Ffaqs-about-flis-open-letter-calling-for-a-pause-on-giant-ai-experiments%2F.png" />FAQ</a></li>
<li><a
href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"
data-inReference="" data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Ffutureoflife.org%252Fopen-letter%252Fpause-giant-ai-experiments%252F.png"
class="favicon"
alt="/favicon/https%3A%2F%2Ffutureoflife.org%2Fopen-letter%2Fpause-giant-ai-experiments%2F.png" />Pause
Giant AI Experiments: An Open Letter</a></li>
<li><a href="http://openai.com/" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/http%253A%252F%252Fopenai.com%252F.svg" class="favicon"
alt="/favicon/http%3A%2F%2Fopenai.com%2F.svg" />Open AI</a></li>
<li><a href="https://lifearchitect.ai/timeline" data-inReference=""
data-noLinkPreview=""><img
src="/favicon/https%253A%252F%252Flifearchitect.ai%252Ftimeline.ico"
class="favicon"
alt="/favicon/https%3A%2F%2Flifearchitect.ai%2Ftimeline.ico" />recent
advancements</a></li>
<li><a href="/image/ai-danger-by-dalle-2.png" data-inReference=""
data-noLinkPreview=""><img src="/favicon.ico" class="favicon"
alt="/favicon.ico" />illustration of "AI danger" by Dalle 2</a></li>
</ul>
<h1 id="citations">Citations</h1>
<ul class="task-list">
</ul></div>
    </main>
    <footer>This is the footer as a template!</footer>  </body>
</html>
